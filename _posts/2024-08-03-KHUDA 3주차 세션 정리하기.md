---
layout: post
title:  "KHUDA 3주차 세션 정리하기"
date:   2024-08-03 14:47:46 +0900
categories: jekyll update
---
# Chapter 03

## 03-01

### 들어가기

+ **지도 학습 알고리즘**
    1. 분류
    2. **회귀**

**회귀**란 클래스 중 하나로 분류하는 것이 아니라 임의의 어떤 숫자를 예측하는 문제(두 변수 사이의 상관관계를 분석하는방법)

**k-최근접 이웃 회귀**는 분류와 똑같이 예측하려는 샘플에 가장 가까운 샘플 k개를 선택하지만 샘플의 타깃은 임의의 수치이다. 이웃한 샘플의 타깃값의 평균을 통해 예측한다.



+ **과대적합**
    + 훈련 세트에서 점수가 굉장히 좋았는데 테스트 세트에서는 점수가 굉장히 나쁘다면 모델이 훈련 세트에 *과대적합*되었다고 함
    + 훈련세트에만 잘 맞는 모델이라 테스트 세트와 나중에 실전에 투입하여 새로운 샘플에 대한 예측을 만들 때 잘 동작하지 않을 것임
    + 모델을 덜 복잡하게 만들어야 함으로 k-최근접 이웃의 경우 k값을 늘린다
+ **과소적합**
    + 훈련 세트보다 테스트 세트의 점수가 높거나 두 점수가 모두 낮은 경우에 훈련세트에 *과소적합*되었다고 함
    + 모델이 너무 단순하여 훈련 세트에 적절히 훈련되지 않은 경우
    + 모델을 복잡하게 만들어야 함으로 k-최근접 이웃의 경우 k값을 줄인다
    + 훈련 세트가 전체 데이터를 대표한다고 가정하기 때문에 훈련 세트를 잘 학습하는 것이 중요함

### 실습
+ step 1. 데이터 준비 및 산점도 그리기
    ```python
    #http://bit.ly/perch_data 이용
    import numpy as np
    import matplotlib.pyplot as plt
    plt.scatter(perch_length, perch_weight)
    plt.xlabel('length')
    plt.ylabel('weight')
    plt.show() #농어의 길이가 커짐에 따라 무게가 늘어남
    ```
+ step 2. 2차원 배열로 훈련 세트와 테스트 세트로 나누기
    ```python
    from sklearn.model_selection import train_test_split
    train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state=42)

    #사이킷런에 사용할 훈련 세트는 2차원 배열이여야함(현재 1차원 배열)
    train_input = train_input.reshape(-1,1)
    test_input = test_input.reshape(-1,1)
    #행(row)의 위치에 -1을 넣고 열의 값을 지정(1로 지정함)해주면 변환될 배열의 행의 수는 알아서 지정이 된다는 뜻
    ```
    + reshape() 메서드는 쿠기가 바뀐 새로운 배열을 반환할 때 지정한 크기가 원본 배열에 있는 원소의 개수와 다르면 에러가 발생함

+ step 3.결정계수
    ```python
    from sklearn.neighbors import KNeighborsRegressor
    knr = KNeighborsRegressor()
    knr.fit(train_input, train_target)
    print(knr.score(test_input, test_target))
    ```
    + 회귀에서는 정확한 숫자를 맞힌다는 것은 거의 불가능함(예측하는 값이나 타깃 모두 임의의 수치라서)
    + 먼저 타깃과 예측한 값의 차이를 제곱 -> 타깃과 타깃 평균의 차이를 제곱한 값으로 나눔
    + 1에 가까울수록 정확도가 높음

+  step 4. mean_absolute_error를 통해 오차 계산
    ```python
    from sklearn.metrics import mean_absolute_error

    #예측값
    test_prediction = knr.predict(test_input)

    #테스트 세트에 대한 평균 절댓값 오차를 계산
    mae = mean_absolute_error(test_target, test_prediction)
    print(mae)
    ```
    + 결정 계수는 직감적으로 얼마나 좋은지 이해하기 어렵다.
    + mean_absolute_error는 타깃과 예측의 절댓값 오차를 평균하여 반환
+ step 5. 과소적합 없애기
    ```python
    knr.n_neighbors=3
    knr.fit(train_input, train_target)
    print(knr.score(train_input, train_target))

    print(knr.score(test_input, test_target))
    ```
    + 훈련세트로 score()메서드를 이용할 경우 결괏값이 테스트 세트로 이용할 때보다 낮음, 즉 과소적합이다
    + 이웃의 개수 k를 줄임
    + 이웃의 개수를 줄이면 훈련 세트에 있는 국지적인 패턴에 민감해지고 이웃의 개수를 늘리면 데이터 전반에 있는 일반적인 패턴을 따름

## 03-02

### 들어가기
k-최근접 이웃 회귀를 사용해서 농어의 무게를 예측했을 때 발생하는 큰 문제는 *훈련 세트 범위 밖의 샘플을 예측할 수 없음*

k-최근접 이웃 회귀는 아무리 멀리 떨어져 있더라도 무조건 가장 가까운 샘플의 타깃을 평균하여 예측

**선형 회귀**란 훈련 세트에 잘 맞는 직선의 방정식을 찾는 것
+ 사이킷런의 LinearRegression 클래스를 사용하면 k-최근접 이웃 알고리즘을 사용했을 때와 동일한 방식으로 모델을 훈련하고 예측 가능
    + fit(), score(), predict()  메서드가 존재함

+ 직선을 그리기 위해서 기울기와 절편이 있어야함
    + coef_와 intercept_속성에 저장되어 있음

**다항 회귀**란 다항식을 사용하여 특성과 타깃 사이의 관계를 나타내는 것

+ 2차 방정식 그래프를 찾기 위해 훈련 세트에 제곱 항을 추가
+ 타깃값은 그대로 사용함

### 실습
+ step1. 데이터 준비하기
    ```python
    #http://bit.ly/perch_data 이용
    from sklearn.model_selection import train_test_split
    
    train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state=42)

    train_input = train_input.reshape(-1,1)
    test_input = test_input.reshape(-1,1)
    ```

+ step 2. 선형 회귀 이용하여 산점도 그리기
    ```python
    from sklearn.linear_model import LinearRegression
    lr=LinearRegression()

    lr.fit(train_input, train_target)

    print(lr.predict([[50]])) #[1241.83860323]

    print(lr.coef_, lr.intercept_)

    # 훈련 세트의 산점도
    plt.scatter(train_input, train_target)
    # 15에서 50까지 1차 방정식 그래프
    plt.plot([15, 50], [15*lr.coef_+lr.intercept_, 50*lr.coef_+lr.intercept_])
    # 50cm 농어 데이터
    plt.scatter(50, 1241.8, marker='^')
    plt.xlabel('length')
    plt.ylabel('weight')
    plt.show()  
    ```
    + ![스크린샷 2024-08-05 170558](https://github.com/user-attachments/assets/bd9c36a4-d12e-46c5-b247-ec59cb6b5511)
    + coef_, intercept_를 통해 직선 방정식의 기울기, 절편을 구함

    + coef_, intercept_를 머신러닝 알고리즘이 찾은 값이라는 의미에서 **모델 파라미터**라고 함

    + 훈련 세트의 점수도 그다지 높지 않으므로 전체적으로 과소적합이라고 볼 수 있음

    + 그래프 왼쪽 아래로 직선이 쭉 뻗어 있는데 직선대로 예측하면 농어의 무게가 0g이하로 내려갈 수 있음

+ step 3. 다항 회귀 이용하여 산점도 그리기
    ```python
    #2차 방정식에 필요한 제곱값을 추가
    train_poly = np.column_stack((train_input**2, train_input)) #train_input의 왼쪽 열에 추가
    
    test_poly = np.column_stack((test_input**2, test_input))

    lr = LinearRegression()
    lr.fit(train_poly, train_target)

    print(lr.predict([[50**2, 50]]))#[1573.98423528]

    print(lr.coef_, lr.intercept_)

    # 구간별 직선을 그리기 위해 15에서 49까지 정수 배열을 만듬(1씩 짧게 끊어서 그림)
    point = np.arange(15, 50)
    # 훈련 세트의 산점도
    plt.scatter(train_input, train_target)
    # 15에서 49까지 2차 방정식 그래프
    plt.plot(point, 1.01*point**2 - 21.6*point + 116.05)
    # 50cm 농어 데이터
    plt.scatter([50], [1574], marker='^')
    plt.xlabel('length')
    plt.ylabel('weight')
    plt.show()
    ```
    + ![스크린샷 2024-08-05 171333](https://github.com/user-attachments/assets/a2f4c365-8eaa-454b-81bb-457df9635afd)
    + 농어의 길이와 무게에 대한 산점도를 보면 조금 구부러진 곡선에 가까움
    + 2차 방정식의 그래프를 그리려면 길이를 제곱한 항이 훈련 세트에 추가되어야 함(타깃값은 그대로)
    + 짧은 직선을 이어서 그리면 마치 곡선처럼 표현 가능

## 03-03
### 들어가기
**다중 회귀**란 여러 개의 특성을 사용한 선형 회귀
+ 길이와 길이 제곱 뿐만 아니라 두께, 높이도 포함
+ ![KakaoTalk_20240805_174657419](https://github.com/user-attachments/assets/b2149ef9-7556-40f4-aec3-580e4c6c529c)
+ 특성이 2개면 타깃값과 함께 3차원 공간을 형성
**특성 공학**이란 기존의 특성을 사용해 새로운 특성을 뽑아내는 작업

**변환기**란 사이킷런은 특성을 만들거나 전처리하기 위한 다양한 클래스를 제공하는데 이러한 클래스를 *변환기*라고 함
+ fit(), transform() 메서드를 제공

**규제**는 머신러닝 모델이 훈련 세트를 너무 과도하게 학습하지 못하도록 훼방하는 것
+ 특성을 많이 추가하면 선형 회귀는 매우 강력한 성능을 내지만 과도하게 많으면 선형 회귀 모델을 제약하기 위한 도구가 필요함
+ 모델이 훈련 세트에 과대적합되지 않도록 하는 것
+ 선형 회귀 모델의 경우 특성에 곱해지는 계수(기울기)의 크기를 작게 만드는 일
+ **릿지** : 계수를 제곱한 값을 기준으로 규제를 적용
+ **라쏘** : 계수의 절댓값을 기준으로 규제를 적용
+ 두 알고리즘 모두 계수의 크기를 줄이지만 라쏘는 아에 0으로 만들 수 있음

**하이퍼파라미터**란 머신러닝 모델이 학습할 수 없고 알려줘야 하는 파라미터
+ ex) alpha값
+ 사이킷런과 같은 머신러닝 라이브러리에서 하이퍼파라미터는 클래스와 메서드의 매개변수로 표현

### 실습
+ step 1. 데이터 준비하기
    ```python
    import pandas as pd
    df = pd.read_csv('https://bit.ly/perch_csv_data')
    perch_full = df.to_numpy() #넘파이 배열로 변환
    print(perch_full)

    import numpy as np
    perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0,
       115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0,
       150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0,
       218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0,
       556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0,
       850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0,
       1000.0])

    from sklearn.model_selection import train_test_split
    train_input, test_input, train_target, test_target = train_test_split(perch_full, perch_weight, random_state=42)
    ```
    + ![KakaoTalk_20240805_175252098](https://github.com/user-attachments/assets/fc2f12f4-c833-4c92-a54e-244b0dbcffe8)
    + 판다스는 유명한 데이터 분석 라이브러리
    + 데이터 프레임은 판다스의 핵심 데이터 구조
    + 인터넷에서 데이터를 바로 다운로드하여 사용가능

+ step 2. 사이킷런의 변환기 이용하기
    ```python
    from sklearn.preprocessing import PolynomialFeatures
    poly = PolynomialFeatures(include_bias=False)
    poly.fit(train_input)
    train_poly = poly.transform(train_input)
    
    print(train_poly.shape) #(42,9)

    poly.get_feature_names() #9개의 특성이 각각 어떤 입력의 조합으로 만들어졌는지 알려줌
    test_poly = poly.transform(test_input)
    ```
    + 변환기는 입력 데이터를 변환하는데 타깃 데이터가 필요하지 않음
    + train_input이 3개의 특성을 가지고 있으므로 PolynomialFeatures을 통해 각 특성을 제곱하고 특성들끼리 곱하여 새로운 특성 6가지 포함 9개가 나옴 
    + include_bias=False를 지정하여 (길이, 높이, 두께,1)로 되는 것에서 1을 없앨 수 있다.

+ step 3. 다중 회귀 모델 훈련하기
    ```python
    from sklearn.linear_model import LinearRegression

    lr = LinearRegression()
    lr.fit(train_poly, train_target)
    print(lr.score(train_poly, train_target))#0.9903183436982125
    print(lr.score(test_poly, test_target))#0.9714559911594111
    ```
    + 과소적합 문제 해결
    ```python
    poly = PolynomialFeatures(degree=5, include_bias=False)

    poly.fit(train_input)
    train_poly = poly.transform(train_input)
    test_poly = poly.transform(test_input)
    print(train_poly.shape) #(42,55); 특성의 개수 55개
    
    lr.fit(train_poly, train_target)
    print(lr.score(train_poly, train_target))
    print(lr.score(test_poly, test_target))#-144.40579436844948
    ```
    + 특성의 개수를 크게 늘리면 선형 모델은 아주 강력해짐
    + 훈련 세트에 대해 거의 완벽하게 학습
    + 훈련 세트에 너무 과대적합되므로 테스트 세트에서는 점수가 낮음

+ step 4. 정규화 이용하기(표준점수)
    ```python
    from sklearn.preprocessing import StandardScaler

    ss = StandardScaler()
    ss.fit(train_poly)

    train_scaled = ss.transform(train_poly)
    test_scaled = ss.transform(test_poly)
    ```
    + ![KakaoTalk_20240805_181044234](https://github.com/user-attachments/assets/dfa6dd5e-0f8e-4348-a0e0-2617ee5acf5d)
    + 특성의 스케일이 정규화되지 않으면 곱해지는 계수 값도 차이 나게 됨

+ step 5. 릿지(Ridge)이용하기
    ```python
    from sklearn.linear_model import Ridge

    ridge = Ridge()
    ridge.fit(train_scaled, train_target)
    print(ridge.score(train_scaled, train_target))#0.9896101671037343
    print(ridge.score(test_scaled, test_target))#0.9790693977615387
    ```
    ```python
    #적절한 알파값 찾기
    import matplotlib.pyplot as plt

    train_score = []
    test_score = []
    alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]
    for alpha in alpha_list:
        # 릿지 모델 만듬
        ridge = Ridge(alpha=alpha)
        # 릿지 모델 훈련
        ridge.fit(train_scaled, train_target)
        # 훈련 점수와 테스트 점수를 저장
        train_score.append(ridge.score(train_scaled, train_target))
        test_score.append(ridge.score(test_scaled, test_target))

    plt.plot(np.log10(alpha_list), train_score)
    plt.plot(np.log10(alpha_list), test_score)
    plt.xlabel('alpha')
    plt.ylabel('R^2')
    plt.show()
    ```
    + ![스크린샷 2024-08-05 213615](https://github.com/user-attachments/assets/6fa425e4-db2c-48b2-b1a9-0f8f58733e42)
    + 적절한 alpha 값은 두 그래프가 가장 가깝고 테스트 세트의 점수가 가장 높은 -1, 즉 = 0.1
    ```python
    ridge = Ridge(alpha=0.1)
    ridge.fit(train_scaled, train_target)

    print(ridge.score(train_scaled, train_target))#0.9903815817570367
    print(ridge.score(test_scaled, test_target))#0.9827976465386928
    ```
    + sklearn.linear_modeal 패키지 안에 있음
    + 규제의 양을 임의로 조절 가능(모델 객체를 만들 때 alpha매개변수로 규제의 강도를 조절)
    + alpha 값이 크면 규제 강도 세지므로 계수 값을 더 줄이고 조금 더 과소 적합되도록 유도
    + alpha 값이 작으면 계수를 줄이는 역할이 줄어들고 선형 회귀 모델과 유사해지므로 과대적합될 가능성이 큼

+ step 6. 라쏘(Lasso)이용하기
    ```python
    from sklearn.linear_model import Lasso

    lasso = Lasso()
    lasso.fit(train_scaled, train_target)
    print(lasso.score(train_scaled, train_target)) #0.989789897208096
    print(lasso.score(test_scaled, test_target)) #0.9800593698421883
    ```
    ```python
    # 알파 찾기
    train_score = []
    test_score = []

    alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]
    for alpha in alpha_list:

        lasso = Lasso(alpha=alpha, max_iter=10000)
        lasso.fit(train_scaled, train_target)
        train_score.append(lasso.score(train_scaled, train_target))
        test_score.append(lasso.score(test_scaled, test_target))
    # 그래프
    plt.plot(np.log10(alpha_list), train_score)
    plt.plot(np.log10(alpha_list), test_score)
    plt.xlabel('alpha')
    plt.ylabel('R^2')
    plt.show()
    ```
    + ![스크린샷 2024-08-05 214107](https://github.com/user-attachments/assets/29b37b85-c854-4943-8a1a-227e8d4513db)
    + 적절한 alpha값은 1, 즉 10
    ```python
    lasso = Lasso(alpha=10)
    lasso.fit(train_scaled, train_target)

    print(lasso.score(train_scaled, train_target))#0.9888067471131867
    print(lasso.score(test_scaled, test_target))#0.9824470598706695
