---
layout: post
title:  "KHUDA 6주차 세션 정리하기"
date:   2024-08-27 14:49:46 +0900
categories: jekyll update
---
# Chapter 06
## 06-01
### 들어가기
**비지도 학습**
+ 타깃이 없을 때 사용하는 머신러닝 알고리즘

**plt.subplots()**
+ 여러 개의 그래프를 배열처럼 쌓을 수 있음
+ subplots()의 두 매개변수는 그래프를 쌓을 행과 열을 지정

**군집**
+ 비슷한 샘플끼리 그룹으로 모으는 작업
+ 대표적인 비지도 학습 작업 중 하나
+ 군집 알고리즘에서 만든 그룹을 **클러스터**

### 실습
+ step 1. 과일 사진 데이터 준비하기
    ```python
    !wget https://bit.ly/fruits_300 -O fruits_300.npy

    import numpy as np
    import matplotlib.pyplot as plt

    fruits = np.load('fruits_300.npy')
    print(fruits.shape)
    #(300, 100, 100)
    print(fruits[0, 0, :])
    ```
    + 과일 데이터는 사과, 바나나, 파인애플을 담고 있는 흑백 사진
    + npy파일을 로드하는 방법은 load()메서드에 파일 이름을 전달
    + 첫번째는 샘플 갯수, 두번째는 차원은 이미지 높이, 세번째 차원은 이미지 너비
    + 각 픽셀은 넘파이 배열의 원소 하나에 대응(즉 배열의 크기는 100 * 100)
    ```python
    plt.imshow(fruits[0], cmap = 'gray')
    plt.show()
    ```
    + ![스크린샷 2024-08-27 145700](https://github.com/user-attachments/assets/ae238361-5d27-4a64-8320-365e93986397)
    + 0에 가까울수록 검게 나타나고 높은 값은 밝게 표시
    원래 흑백 사진은 물체가 검은색이고 바탕이 흰색이지만 사진으로 찍은 이미지를 넘파이 배열로 변환할 때 자동으로 반전시킴
    + 반전하는 이유는 원래대로 출력하면 바탕이 255에 가까워져서 물체에 의미를 부여하지 않고 바탕에 의미부여하게 되기 때문임
+ step 2. 픽셀값 분석하기
    ```python
    #1차원 배열로 변환하기
    apple = fruits[0:100].reshape(-1, 100*100)
    pineapple = fruits[100:200].reshape(-1, 100*100)
    banana = fruits[200:300].reshape(-1, 100*100)

    print(apple.shape)
    #(100,10000)
    ```
    + 1차원 배열로 바꾸면 이미지로 출력하긴 어렵지만 배열을 계산할 때 편리
    + np.reshape()에서 첫 번째 차원을 -1로 지정하면 자동으로 남은 차원을 할당함
    ```python
    # 각 변수 배열에 들어 있는 샘플의 픽셀 평균값을 계산해보기
    print(apple.mean(axis=1))
    print(pineapple.mean(axis=1))
    print(banana.mean(axis=1))
    ```
    + 샘플은 모두 가로로 값을 나열했으니 axis=1로 지정하여 평균을 계산
    + axis=0으로 하면 첫 번째 축인 행을 따라 계산함
    ```python
    # 각 픽셀의 평균값 구하기
    # 즉 100*100형태의 샘플이 100개 있었으니 100개의 1*1, 1*2...의 픽셀의 평균 구하기
    apple_mean = np.mean(apple, axis=0).reshape(100,100)
    pineapple_mean = np.mean(pineapple, axis=0).reshape(100,100)
    banana_mean = np.mean(banana, axis=0).reshape(100,100)

    fig, axs = plt.subplots(1,3, figsize=(20,5))
    axs[0].imshow(apple_mean, cmap='gray_r')
    axs[1].imshow(pineapple_mean, cmap='gray_r')
    axs[2].imshow(banana_mean, cmap='gray_r')
    plt.show()
    ```
    + 픽셀 평균값을 100*100 크기로 바꿔서 이미지처럼 출력하면 좋음
    + 픽셀을 평균 낸 이미지를 모든 사진을 합쳐 놓은 대표 이미지로 생각할 수 있음
+ step 3. 평균값과 가까운 사진 고르기
    ``` python
    add_diff=np.abs(fruits - apple_mean)
    abs_mean=np.mean(add_diff, axis=(1,2))
    print(abs_mean.shape)
    #(300,)
    ```
    + 각 샘플에 대한 평균을 구하기 위해 axis에 두 번째, 세 번째 차원을 모두 지정
    + 이렇게 계산한 abs_mean은 각 샘플의 오차 평균이므로 크기가 (300,)인 1차원 배열
    + 절댓값 오차를 나타낸 것
    ```python
    apple_index=np.argsort(abs_mean)[:100]
    fig, axs=plt.subplots(10,10, figsize=(10,10))
    for i in range(10):
        for j in range(10):
            axs[i,j].imshow(fruits[apple_index[i*10+j]], cmap='gray_r')
            axs[i,j].axis('off') #깔끔하게 이미지만 그리기 위해 axis('off')를 사용하여 좌표축을 그리지 않음
    plt.show()
    ```
    + ![KakaoTalk_20240827_152222834](https://github.com/user-attachments/assets/92dcc152-19a7-450d-9085-169a68cf188d)
    + apple_mean과 오차가 가장 작은 샘플 100개 고르기
    + np.argsort()함수는 작은 것에서 큰 순서대로 나열한 abs_mean 배열의 인덱스를 반환

## 06-02
### 들어가기
**k-평균 알고리즘**
1. 무작위로 k개의 클러스터 중심을 정함
2. 각 샘플에서 가장 가까운 클러스터 중심을 찾아 해당 클러스터의 샘플로 지정
3. 클러스터에 속한 샘플의 평균값으로 클러스터 중심을 변경
4. 클러스터 중심에 변화가 없을 때까지 2번 돌아가 반복
+ 처음에는 랜덤하게 클러스터 중심을 선택하고 점차 가까운 샘플의 중심으로 이동하는 비교적 간단한 알고리즘
+ k-평균 군집 알고리즘은 평균값을 자동으로 찾아줌
+ 이 평균값은 클러스터의 중심에 위치하기 때문에 *클러스터 중심* 또는 *센트로이드*라고 불림

**클러스터 중심**
+ k-평균 알고리즘이 만든 클러스터에 속한 샘플의 특성 평균값
+ 가장 가까운 클러스터 중심을 샘플의 또 다른 특성으로 사용하거나 새로운 샘플에 대한 예측으로 활용가능

**엘보우 방법**
+ 최적의 클러스터 개수를 정하는 방법 중 하나
+ *이너셔*는 클러스터 중심과 샘플 사이 거리의 제곱 합
+ 클러스터 개수에 따라 이너셔 감소가 꺾이는 지점이 적절한 클러스터 개수 k가 될 수 있음
+ *이너셔*는 클러스터에 속한 샘플이 얼마나 가깝게 모여 있는지를 나타내는 값으로 생각할 수 있음
+ 그래프의 모양을 따서 엘보우 방법이라 함



### 실습
+ step 1. KMeans 클래스
    ```python
    fruits_2d = fruits.reshape(-1, 100*100)
    #모델을 훈련하기 위해 (샘플 개수, 너비*높이)크기를 가진 2차원 배열로 변경

    from sklearn.cluster import KMeans

    km = KMeans(n_clusters=3, random_state=42)
    km.fit(fruits_2d)
    ```
    + 매개변수는 클러스터 개수를 지정하는 n_clusters
    + 비지도 학습이기 때문에 타깃 데이터를 사용하지 않음

    ```python
    print(km.labels_)
    #[0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 2 2 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2
    2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
    ```
    + KMeans 클래스 객체의 labels_속성에 저장됌
    + labels_길이는 샘플 개수로 각 샘플이 어떤 레이블에 해당되는지 나타냄
    + n_cluster=3으로 지정했기 때문에 labels_ 배열의 값은 0,1,2 중 하나
    ```python
    print(np.unique(km.labels_, return_counts=True)) #np.unique는 중복된 값을 지워줌
    #(array([0, 1, 2], dtype=int32), array([ 91,  98, 111]))
    ```
    + 첫 번째 클러스터(레이블 0)가 91개
    + 두 번째 클러스터(레이블 1)가 98개
    + 세 번째 클러스터(레이블 2)가 111개

    ```python
    import matplotlib.pyplot as plt

    def draw_fruits(arr, ratio=1):
    n = len(arr)
    rows = int(np.ceil(n/10))
    cols = n if rows < 2 else 10

    fig, axs = plt.subplots(rows, cols, figsize=(cols*ratio, rows*ratio), squeeze=False)
    for i in range(rows):
        for j in range(cols):
        if i*10+j < n:
            axs[i, j].imshow(arr[i*10+j], cmap='gray_r')
        axs[i, j].axis('off')
    plt.show()

    draw_fruits(fruits[km.labels_==0])
    draw_fruits(fruits[km.labels_==1])
    draw_fruits(fruits[km.labels_==2])
    ```
    + 각 클러스터가 어떤 이미지를 나타냈는지 그림으로 출력하기 위해 간단한 유틸리티 함수 작성

+ step 2. 클러스터 중심
    ```python
    draw_fruits(km.cluster_centers_.reshape(-1,100,100), ratio=3)
    ```
    + 클러스터 중심은 cluster_centers_속성에 저장됌
    + 이 배열은 fruits_2d 샘플의 클러스터 중심이기 때문에 이미지로 출력하려면 100*100크기의 2차원 배열로 바꿔야함
    ```python
    print(km.transform(fruits_2d[100:101]))
    #[[5267.70439881 8837.37750892 3393.8136117 ]]
    ```
    + KMeans 클래스는 훈련 데이터 샘플에서 클러스터 중심까지 거리로 변환해주는 transform()메서드를 가지고 있음
    + 100번째 인덱스 샘플은 세번째 클러스터에 가장 가까움
    + 2차원 배열이라 [100]이 아닌 [100:101]로 써야함
    ```python
    print(km.n_iter_)
    # 4
    ```
    + 클러스터 중심을 옮기면서 최적의 클러스터를 찾는 횟수

+ step 3. 최적의 k 찾기

    + k-평균 알고리즘의 단점 중 하나는 클러스터 개수를 사전에 지정해야 한다는 것
    + 적절한 클러스터 개수를 찾기 위해 대표적인 방법인 *엘보우 방법*이 있음
    + *엘보우 방법*은 클러스터 개수를 늘려가면서 이너셔의 변화를 관찰하여 최적의 클러스터 개수를 찾는 방법
    + 클러스터 개수를 증가시키면서 이너셔를 그래프로 그리면 감소하는 속도가 꺾이는 지점이 있는데 이 지점이 최적의 k
    + ![KakaoTalk_20240827_174439722](https://github.com/user-attachments/assets/63bdc609-e05d-48bf-8023-9cc207fecfc8)
    ```python
    inertia=[]
    for k in range(2,7):
        km = KMeans(n_clusters=k, random_state=42)
        km.fit(fruits_2d)
        inertia.append(km.inertia_)
    plt.plot(range(2,7), inertia)
    plt.xlabel('k')
    plt.ylabel('inertia')
    plt.show()
    ```
    + ![스크린샷 2024-08-27 175139](https://github.com/user-attachments/assets/cd9d0536-7e3c-4dd9-a5a1-7b5188d685c8)
    + KMeans 클래스는 자동으로 이너셔를 계산해서 inertia_속성으로 제공함
    + 다음 코드에서 클러스터 개수k를 2~6까지 바꿔가며 KMeans 클래스를 5번 훈련함
    + fit() 메서드로 모델을 훈련한 후 inertia_속성에 저장된 이너셔 값을 inertia리스트에 추가함
    + 마지막으로 inertia리스트에 저장된 값을 그래프로 출력
    + k=3에서 그래프의 기울기가 조금 바뀜
## 06-03
### 들어가기

데이터가 가진 속성을 *특성*이라 함. 예를 들면 과일 사진의 경우 10000개의 픽셀이 있기 때문에 10000개의 특성이 있는 것

머신러닝에서 이런 특성은 *차원*이라 부를 것임

**차원 축소**
+ 원본 데이터의 특성을 적은 수의 새로운 특성으로 변환하는 비지도 학습의 한 종류
+ 저장 공간을 줄이고 시각화하기 쉬움
+ 다른 알고리즘의 성능을 높일 수 있음
+ 데이터를 가장 잘 나타내는 일부 특성을 선택하여 데이터 크기를 줄이고 지도 학습 모델의 성능을 향상시킬 수 있는 방법

**주성분 분석**
+ 차원 축소 알고리즘의 하나로 데이터에서 가장 분산이 큰 방향을 찾는 방법
+ 가장 분산이 큰 방향이 가장 데이터의 분포를 잘 표현하는 방향
+ 이런 방향을 주성분이라고 함
+ 원본 데이터를 주성분에 투영하여 새로운 특성을 만들수 있음
+ 일반적으로 주성분은 원본 데이터에 있는 특성 개수보다 작음
+ 주성분은 원본 차원과 같고 주성분으로 바꾼 데이터는 차원이 줄어듬

**설명된 분산**
+ 주성분 분석에서 주성분이 얼마나 원본 데이터의 분산을 잘 나타내는지 기록한 것
+ 사이킷런의 PCA 클래스는 주성분 개수나 설명된 분산의 비율을 지정하여 주성분 분석을 수행할 수 있음


### 실습
+ step 1. PCA 클래스
    ```python
    from sklearn.decomposition import PCA
    pca=PCA(n_components=50)
    pca.fit(fruits_2d)

    print(pca.components_.shape)
    #(50,10000)

    fruits_pca=pca.transform(fruits_2d)
    print(fruits_pca.shape)
    #(300,50)
    ```
    + n_components 매개변수에 주성분의 개수를 지정해줘야함
    + pca의 transform()메서드를 사용해 원본 데이터의 차원을 50으로 줄임

+ step 2. 원본 데이터 재구성
    ```python
    fruits_inverse=pca.inverse_transform(fruits_pca)
    print(fruits_inverse.shape)
    #(300,10000)

    fruits_reconstruct = fruits_inverse.reshape(-1,100,100)
    for start in [0,100,200]:
        draw_fruits(fruits_reconstruct[start:start+100])
        print("\n")
    ```
    + 최대한 분산이 큰 방향으로 데이터를 투영했기 때문에 우너본 데이터를 상당 부분 재구성할 수 있음
    + 이를 위해 inverse_transform() 메서드를 제공
    + 거의 모든 과일들이 잘 복구됨

+ step 3. 설명된 분산
    ```python
    print(np.sum(pca.explained_variance_ratio_))
    #0.921546235881595
    #원본 데이터의 약 92%의 분산을 유지

    plt.plot(pca.explained_variance_ratio_)
    ```
    + ![스크린샷 2024-08-27 205706](https://github.com/user-attachments/assets/679ae108-f123-4bf9-a0d5-ab67f9425f97)
    + 주성분이 원본 데이터의 분산을 얼마나 잘 나타내는지 기록한 값을 *설명된 분산*이라 함

+ step 4. 다른 알고리즘과 함께 사용하기  
    ```python
    from sklearn.linear_model import LogisticRegression
    
    lr = LogisticRegression()
    target = np.array([0]*100 + [1]*100 + [2]*100)
    
    from sklearn.model_selection import cross_validate
    scores = cross_validate(lr, fruits_2d, target)
    print(np.mean(scores['test_score']), np.mean(scores['fit_time']))
    #0.9966666666666667 2.0119098663330077
    #특성이 10000개나 되기 때문에 300개의 샘플에서는 금방 과대적합된 모델을 만들기 쉬움
    ```
    ```python
    scores = cross_validate(lr, fruits_pca, target)
    print(np.mean(scores['test_score']), np.mean(scores['fit_time']))
    # 1.0 0.03556900024414063
    ```
    + 차원을 줄이니 과대적합도 예방하고 훈련 시간도 더 짧게 걸림
    ```python
    pca= PCA(n_components=0.5)
    pca.fit(fruits_2d)

    print(pca.n_components_)
    #2
    ```
    + n_components 매개변수에 0~1까지를 입력하면 원하는 분산 비율을 설정할 수 있음
    + 2개의 특성만으로 원본 데이터에 있는 분산의 50%를 표현할 수 있음
    ```python
    from sklearn.cluster import KMeans
    km = KMeans(n_clusters=3, random_state=42)
    km.fit(fruits_pca)
    print(np.unique(km.labels_, return_counts=True))
    #(array([0, 1, 2], dtype=int32), array([110,  99,  91]))
    ```
    + 원본 데이터를 사용했을 때와 거의 비슷한 결과
