---
layout: post
title:  "KHUDA 5주차 세션 정리하기"
date:   2024-08-17 16:56:46 +0900
categories: jekyll update
---
# Chapter 05
## 05-01
### 들어가기

**결정트리**
+ 예/아니오에 대한 질문을 이어나가면서 정답을 찾아 학습하는 알고리즘(스무고개와 비슷함)
+ 리프 노드에서 가장 많은 클래스가 예측 클래스가 됨

**불순도**
+ 결정 트리가 최적의 질문을 찾기 위한 기준
+ 지니 불순도와 엔트로피 불순도 제공

**정보 이득**
+ 부모 노드와 자식 노드의 불순도 차이
+ 결정 트리 알고리즘은 정보 이득이 최대화되도록 학습

**가지치기**
+ 결정 트리는 제한 없이 성장하면 과대적합되기 쉬움
+ 결정 트리의 성장을 제한하는 방법임

**특성 중요도**
+ 결정 트리에 사용된 특성이 불순도를 감소하는데 기여한 정도를 나타내는 값
+ 결정 트리의 장점임

### 실습
+ step 1. 데이터 준비하기
    ```python
    import pandas as pd
    wine = pd.read_csv('https://bit.ly/wine_csv_data')
    wine.head()

    wine.info()
    wine.describe()
    ```
    + info()메소드는 각 열의 데이터 타입과 누락된 데이터가 있는지 확인하는 용도
    + describe()메소드는 열에 대한 간략한 통계를 출력
    ```python
    data = wine[['alcohol','sugar','pH']].to_numpy()
    target = wine['class'].to_numpy()

    from sklearn.model_selection import train_test_split
    train_input, test_input, train_target, test_target=train_test_split(data, target, test_size=0.2, random_state=42) #테스트 세트를 20프로만 놔둠

    from sklearn.preprocessing import StandardScaler
    ss = StandardScaler()
    ss.fit(train_input)
    train_scaled = ss.transform(train_input)
    test_scaled = ss.transform(test_input)
    ```
+ step 2. 결정 트리로 나타내기
    ```python
    from sklearn.tree import DecisionTreeClassifier
    dt=DecisionTreeClassifier(random_state=42)
    dt.fit(train_scaled, train_target)
    print(dt.score(train_scaled, train_target))
    #0.996921300750433
    print(dt.score(test_scaled, test_target))
    #0.8592307692307692
    ```
    + 사이킷런의 DecisionTreeClassifier 클래스를 사용해 결정 트리 모델을 훈련
    ```python
    import matplotlib.pyplot as plt
    from sklearn.tree import plot_tree
    plt.figure(figsize=(10,7))
    plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol','sugar','pH'])
    plt.show()
    ```
    + ![스크린샷 2024-08-17 160406](https://github.com/user-attachments/assets/301bda83-0196-444b-aff0-31b3159325b0)
    + 사이킷런은 plot_tree()함수를 사용해 결정 트리를 이해하기 쉬운 트리 그림으로 출력함
    + 노드는 훈련 데이터의 특성에 대한 테스트를 표현함
    + 맨 위의 노드 = 루트 노드, 맨 아래 노드 = 리프 노드
    + max_depth=1을 통해 루트 노드를 제외하고 하나의 노드를 더 확장하여 그림
    + filled를 통해 클래스에 맞게 노드의 색을 칠할 수 있음

+ step 3. 결정트리 이해하기
    + ![KakaoTalk_20240817_160847659](https://github.com/user-attachments/assets/f3b28ed8-67b6-441e-a7d6-f8d20df51f5b)
    
        + 루트 노드는 당도가 -0.239이하인지 질문
        + 이하이면 왼쪽 가지, 아니면 오른쪽 가지
        + 훈련 세트의 샘플 수가 5197개여서 루트 노드의 샘플 수는 5197개
        + 이 중에서 1258개가 음성클래스(레드 와인), 3939개가 양성클래스(화이트 와인)
        + filled = True로 지정하면 클래스마다 색깔을 부여하고 어떤 클래스의 비율이 높으면 점점 진한색으로 표시

    + 불순도 이해하기
        + gini는 지니 불순도를 의미
        + criterion 매개변수의 기본값이 'gini'
        + 지니 불순도 = 1 - (음성 클래스 비율 ^2 + 양성 클래스 비율 ^2)

    + 정보 이득 이해하기
        + 결정 트리 모델은 부모 노드와 자식 노드의 불순도 차이가 가능한 크도록 트리를 성장시킴
        + ![KakaoTalk_20240817_162200622](https://github.com/user-attachments/assets/eb33e8b9-0830-4af7-b56b-e4ff6b8d2f50)
        + 부모와 자식 노드 사이의 불순도 차이가 *정보 이득*임
        + 결정 트리 알고리즘은 정보 이득이 최대가 되도록 데이터를 나눔
    
+ step 4. 가지치기 하기
    ```python
    dt = DecisionTreeClassifier(max_depth=3, random_state=42)
    dt.fit(train_input, train_target)
    print(dt.score(train_input, train_target))
    #0.8454877814123533
    print(dt.score(test_input, test_target))
    #0.8415384615384616
    ```
    + 불순도는 클래스별 비율을 가지고 계산하는데 샘플을 어떤 클래스 비율로 나누는지 계산할 때 특성값의 스케일은 계산에 영향을 미치지 않음
    + 따라서 표준화 전처리를 할 필요 없음
    + 특성값을 표준점수로 바꾸지 않아서 이해하기 훨씬 쉬움
    ```python
    plt.figure(figsize=(10,7))
    plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
    plt.show()
    ```
    + ![KakaoTalk_20240817_164557733](https://github.com/user-attachments/assets/880b1c3a-de7a-45ac-b114-9c2fcb4bc644)
    ```python
    print(dt.feature_importances_)
    #[0.12345626 0.86862934 0.0079144 ]
    ```
    + 결정 트리는 어떤 특성이 가장 유용한지 나타내는 특성 중요도를 계산해줌
    + 트리의 루트 노드와 깊이 1에서 당도를 사용했기 때문에 당도가 가장 유용한 특성 중 하나

## 05-02
### 들어가기
테스트 세트로 일반화 성능을 올바르게 예측하려면 가능한 한 테스트 세트를 사용하지 말아야 함

레드 와인과 화이트 와인을 선별하는 작업의 성능을 끌어올리기 위해 결정 트리의 다양한 하이퍼파라미터를 시도해봐야함

**하이퍼 파라미터**
+ 사용자 지정 파라미터
+ 하이퍼 파라미터 튜닝
    1. 라이브러리가 제공하는 기본값을 그대로 사용
    2. 검증 세트의 점수나 교차 검증을 통해서 매개변수를 조금씩 바꾸기
    + ※하나의 매개변수의 값이 달라지면 다른 매개변수의 최적값도 달라짐
    + 이를 해결하기 위해 *그리드 서치*


**검증세트**
+ 하이퍼파라미터 튜닝을 위해 모델을 평가할 때, 테스트 세트를 사용하지 않기 위해 훈련 세트에서 다시 떼어 낸 데이터 세트

**교차 검증**
+ 훈련 세트를 여러 폴드로 나눈 다음 한 폴드가 검증 세트의 역할을 하고 나머지 폴드에서는 모델을 훈련
+ 이런 식으로 모든 폴드에 대해 검증 점수를 얻어 평균하는 방법
+ ![3폴드 교차검증](https://github.com/user-attachments/assets/14eba5f2-3e1d-4fb5-8e67-5d97fa78672f)
+ 사이킷런의 cross_validate()를 통해 교차 검증 가능

**그리드 서치**
+ 하이퍼파라미터 탐색을 자동화해주는 도구
+ 탐색할 매개변수를 나열하면 교차 검증을 수행하여 가장 좋은 검증 점수의 매개변수 조합을 선택
+ 매개변수 조합으로 최종 모델을 훈련
+ 사이킷런의 GridSearchCV 클래스가 하이퍼파라미터 탐색과 교차 검증을 한번에 수행
1. 탐색할 매개변수를 지정
2. 그 다음 훈련 세트에서 그리드 서치를 수행하여 최상의 평균 검증 점수가 나오는 매개변수 조합을 찾고 이 조합은 그리드 서치 객체에 저장
3. 그리드 서치는 최상의 매개변수에서 전체 훈련 세트를 사용해 최종 모델을 훈련하고 이 모델도 그리드 서치 객체에 저장

**랜덤 서치**
+ 연속된 매개변수 값을 탐색할 때 유용
+ 탐색할 값을 직접 나열하는 것이 아니고 탐색 값을 샘플링할 수 있는 확률 분포 객체를 전달
+ 지정된 횟수만큼 샘플링하여 교차 검증을 수행하기 때문에 시스템 자원이 허락하는 만큼 탐색량을 조절할 수 있음
+ 매개변수의 값이 수치일 때 값의 범위나 간격을 미리 정하기 어려울 수 있을 때 사용
### 실습
+ step 1. 검증세트 만들기
    ```python
    sub_input, val_input, sub_target, val_target = train_test_split(train_input, train_target, test_size=0.2, random_state=42)
    ```
    + train_test_split을 두 번 사용하여 훈련 세트에서 검증 세트를 나눔

+ step 2. 교차 검증하기
    ```python
    from sklearn.model_selection import cross_validate
    scores = cross_validate(dt, train_input, train_target)
    print(scores)
    #{'fit_time': array([0.0255537 , 0.05454206, 0.02650952, 0.02505946, 0.02461195]), 'score_time': array([0.00230098, 0.00608683, 0.00486255, 0.00342655, 0.00189686]), 'test_score': array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])}
    ```
    + *교차 검증*을 통해 안정적인 검증 점수를 얻고 훈련에 더 많은 데이터를 사용할 수 있음
    + cross_validate는 첫 번째 매개변수로 평가할 모델 객체를 줌, 두 번째로 훈련 세트 전체를 함수에 전달
        + cv 매개변수에서 폴드 수를 바꿀 수 있음(기본값은 5폴드)
        + cross_validate()는 훈련 세트를 섞어 폴드를 나누지 않음
        + fit_time, score_time은 모델을 훈련하는 시간, 검증하는 시간을 나타냄
        + test_score은 검증 폴드의 점수
    + 교차 검증을 할 때 훈련 세트를 섞으려면 분할기를 지정해야함
    ```python
    from sklearn.model_selection import StratifiedKFold
    splitter = StratifiedKFold(n_splits=10, shuffle=True, random_state=42) #n_splits 매개변수는 몇 폴드 교차 검증을 할지 정함
    scores = cross_validate(dt, train_input, train_target, cv=splitter)
    print(scores)
    #0.8574181117533719
    ```
    + 훈련 세트를 섞기 위해 분할기를 지정하는 과정
    + 사이킷런 분할기는 교차 검증에서 폴드를 어떻게 나눌지 결정
    + 회귀 모델일 때 KFold 분할기, 분류 모델일 경우 타깃 클래스를 골고루 나누기 위해 StratifiedKFold를 사용

+ step 3. 하이퍼파라미터 튜닝
    ```python
    from sklearn.model_selection import GridSearchCV
    params = {'min_impurity_decrease': np.arange(0.0001, 0.001, 0.0001),
          'max_depth': range(5,20,1),
          'min_samples_split': range(2,100,10)}
    #수행할 모델의 수는 6750개
    gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1) #n_jobs는 병렬 실행에 사용할 CPU 코어 수를 지정하는 것
    gs.fit(train_input, train_target)
    print(gs.best_params_) #.best_params는 각 매개변수의 최적값을 나타내줌
    #{'max_depth': 14, 'min_impurity_decrease': 0.0004, 'min_samples_split': 12}
    ```
    + 그리드 서치는 훈련이 끝나면 훈련한 모델 중에서 검증 점수가 가장 높은 모델의 매개변수 조합으로 전체 훈련 세트에서 자동으로 다시 모델을 훈련함
        + gs 객체의 best_estimator_속성에 저장되어 있음
        + 최적의 매개변수는 best_params_에 저장되어 있음
        + 각 매개변수에서 수행한 교차 검증의 평균 점수는 cv_results_ 속성의 'mean_test_score'키에 저장
+ step 4. 랜덤 서치 사용하기
    ```python
    from scipy.stats import uniform, randint
    params = {'min_impurity_decrease': uniform(0.0001, 0.001),'max_depth': randint(20,50),'min_samples_split': randint(2,25),'min_samples_leaf': randint(1,25)}

    from sklearn.model_selection import RandomizedSearchCV
    gs = RandomizedSearchCV(DecisionTreeClassifier(random_state=42), params, n_iter=100, n_jobs=-1, random_state=42)
    gs.fit(train_input, train_target)
    ```
    + 랜덤 서치에 randint과 uniform 클래스 객체를 넘겨주고 총 몇 번을 샘플링해서 최적의 매개변수를 찾으라고 명령할 수 있음
    + 샘플링 횟수는 RandomizedSearchCV의 n_iter 매개변수에 지정

## 05-02
### 들어가기
**앙사블 학습**
+ 정형 데이터를 다루는 데 가장 뛰어난 성과를 내는 알고리즘
+ 더 좋은 예측 결과를 만들기 위해 여러 개의 모델을 훈련하는 머신러닝 알고리즘

**랜덤 포레스트**
+ 대표적인 결정 트리 기반의 앙상블 학습 방법
+ 결정 트리를 랜덤하게 만들어 결정 트리(나무)의 숲을 만들고 나서 각 결정 트리의 예측을 사용해 최종 예측을 만듦
+ ![KakaoTalk_20240820_154817186](https://github.com/user-attachments/assets/25ad24f5-c91c-4094-8704-5c5e79a56b13)
+ 부트스트랩 샘플을 사용하고 랜덤하게 일부 특성을 선택하여 트리를 만드는 것이 특징
+ 훈련 세트에 과대적합되는 것을 막아주고 검증 세트와 테스트 세트에서 안정적인 성능을 얻을 수 있음

※부트스트랩: 데이터 세트에서 중복을 허용하여 데이터를 샘플링하는 방식 (부트스트랩 샘플은 훈련 세트와 크기가 같음)

**엑스트라 트리**
+ 랜덤 프레스트와 비슷하게 결정 트리를 사용하여 앙상블 모델을 만들지만 부트스트랩 샘플을 사용하지 않음
+ 랜덤하게 노드를 분할해 과대적합을 감소
+ 랜덤하게 노드를 분할하기 때문에 빠른 계산 속도가 엑스트라 트리의 장점

**그레이디언트 부스팅**
+ 랜덤 포레스트나 엑스트라 트리와 달리 결정 트리를 연속적으로 추가하여 손실 함수를 최소화하는 앙상블 방법
+ 깊이가 얕은 결정 트리를 사용하여 이전 트리의 오차를 보완하는 방식으로 앙상블 하는 방법(깊이가 얕은 결정 트리를 이용하기 때문에 과대적합에 강하고 일반적으로 높은 일반화 성능을 기대할 수 있음)
+ 훈련 속도가 조금 느리지만 더 좋은 성능을 기대
+ 그레이디언트 부스팅의 속도를 개선한 것이 *히스토그램 기반 그레이디언트 부스팅*이며 안정적인 결과와 높은 성능으로 인기가 높음

### 실습
1. 랜덤 포레스트 이용하기 
 ```python
from sklearn.model_selection import cross_validate
from sklearn.ensemble import RandomForestClassifier
rf=RandomForestClassifier(n_jobs=-1,random_state=42)

scores=cross_validate(rf,train_input,train_target,return_train_score=True,n_jobs=-1) #return_train_score=True를 지정해주므로 검증 점수뿐만 아니라 훈련 세트에 대한 점수도 같이 반환
print(np.mean(scores['train_score']),np.mean(scores['test_score']))
#0.9973541965122431 0.8905151032797809 과대적합
```
```python
rf.fit(train_input, train_target)
print(rf.feature_importances_)
#[0.23167441 0.50039841 0.26792718]
```
+ 랜덤 포레스트는 결정 트리의 앙상블이기 때문에 DecisionTreeClassifier가 제공하는 중요한 매개변수를 모두 제공
+ 랜덤 포레스트의 특성 중요도는 각 결정 트리의 특성 중요도를 취합한 것
+ 랜덤하게 선택한 샘플과 특성을 사용하기 때문에 훈련 세트에 과대적합되는 것을 막아주고 검증 세트와 테스트 세트에서 안정적인 성능을 얻을 수 있음
+ 특성의 일부를 랜덤하게 선택하여 결정 트리를 훈련하기 때문에 그 결과 하나의 특성에 과도하게 집중하지 않고 좀 더 많은 특성이 훈련에 기여할 기회를 얻음
+ 과대적합을 줄이고 일반화 성능을 높이는데 도움이 됌
```python
rf=RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)
rf.fit(train_input, train_target)
print(rf.oob_score_)
#0.8934000384837406
```
+ 부트스트랩 샘플에 포함되지 않고 남는 샘플들을 OOB라 함
+ 자체적으로 모델을 평가할 수 있음(OOB가 검증세트의 역할을 수행)
2. 엑스트라 트리 이용하기
```python
from sklearn.ensemble import ExtraTreesClassifier
et = ExtraTreesClassifier(n_jobs=-1, random_state=42)
scores = cross_validate(et, train_input, train_target,return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
#0.9974503966084433 0.8887848893166506
```
+ 엑스트라 트리가 무작위성이 좀 더 크기 때문에 랜덤 포레스트보다 더 많은 결정 트리를 훈련해야함
```python
et.fit(train_input, train_target)
print(et.feature_importances_)
#[0.20183568 0.52242907 0.27573525]
```
3. 그레이디언트 부스팅 이용하기
```python
from sklearn.ensemble import GradientBoostingClassifier
gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42)
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
#0.9464595437171814 0.8780082549788999
```
+ 그레이디언트 부스팅은 결정 트리를 계속 추가하면서 가장 낮은 곳을 찾아 이동해야함
+ 그레이디언트 부스팅은 결정 트리의 개수를 늘려도 과대적합에 매우 강함
+ 학습률을 증가시키고 트리의 개수를 늘리면 조금 더 성능이 향상될 수 있음
+ n_estimators는 결정 트리 개수, learning_rate는 학습률
+ n_jobs매개변수가 없어서 훈련 속도 느림
```python
gb.fit(train_input, train_target)
print(gb.feature_importances_)
#[0.15872278 0.68010884 0.16116839]
```
4. 히스토그램 기반 그레이디언트 부스팅 이용하기
```python 
from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoostingClassifier
hgb = HistGradientBoostingClassifier(random_state=42)
scores = cross_validate(hgb, train_input, train_target, return_train_score=True)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
#0.9321723946453317 0.8801241948619236
#과대적합을 잘 억제하면서 조금 더 높은 성능 제공
```
+ 정형 데이터를 다루는 머신러닝 알고리즘 중에 가장 인기가 높은 알고리즘
+ 먼저 입력 특성을 256개의 구간으로 나누고 따라서 노드를 분할할 때 최적의 분할을 매우 빠르게 찾을 수 있음
+ 트리의 개수 지정 대신 부스팅 반복 횟수를 지정하는 max_iter를 사용
```python
hgb.fit(train_input, train_target)
print(rf.feature_importances_)
#[0.23167441 0.50039841 0.26792718]
hgb.score(test_input, test_target)
#0.8723076923076923
```
+ 랜덤 포레스트와 비슷하게 다른 특성에도 관심을 많이 보임
+ 다양한 특성을 골고루 잘 평가한다고 예상할 수 있음

5. XGBoost, LightGBM 이용하기
```python
from xgboost import XGBClassifier
xgb=XGBClassifier(tree_method='hist',random_state=42)
scores=cross_validate(xgb,train_input,train_target,return_train_score=True)
print(np.mean(scores['train_score']),np.mean(scores['test_score']))
#0.9558403027491312 0.8782000074035686
```
+ XGBoost는 다양한 부스팅 알고리즘 지원
+ tree_method='hist'로 지정하면 히스토그램 기반 그레이디언트 부스팅 사용가능
```python
from lightgbm import LGBMClassifier
lgb=LGBMClassifier(random_state=42)
scores=cross_validate(lgb,train_input,train_target,return_train_score=True,n_jobs=-1)
print(np.mean(scores['train_score']),np.mean(scores['test_score']))
```
+ 마이크로소프트에서 만든 LightGBM