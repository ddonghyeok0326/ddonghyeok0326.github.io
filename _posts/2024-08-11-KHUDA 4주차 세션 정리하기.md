---
layout: post
title:  "KHUDA 4주차 세션 정리하기"
date:   2024-08-03 14:47:46 +0900
categories: jekyll update
---
# Chapter 04

## 04-01

### 들어가기
**로지스틱 회귀**
+ 회귀 모델이 아닌 *분류*모델
+ 선형 방정식 사용
+ 값을 0에서 1사이로 압축
+ ![KakaoTalk_20240811_175925283](https://github.com/user-attachments/assets/0a1162b6-4486-4709-a4ca-2068d57addcc)

**다중 분류**
+ 타깃 클래스가 2개 이상인 분류

**시그모이드 함수**
+ 선형 방정식의 출력을 0과 1사이의 값으로 압축하며 이진 분류를 위해 사용

**소프트맥스 함수**는 다중 분류에서 여러 선형 방정식의 출력 겨로가를 정규화하여 합이 1이 되도록 만듦

### 실습
+ step 1. 데이터 준비하기
    ```python
    import pandas as pd
    fish = pd.read_csv('https://bit.ly/fish_csv')
    fish.head() 
    #판다스는 csv 파일의 첫 줄을 자동으로 인식해 열 제목으로 만들어줌

    fish_input = fish[['Weight', 'Length', 'Diagonal', 'Height', 'Width']].to_numpy() #데이터프레임에서 원하는 열을 리스트로 나열하여 들고오기, 입력 데이터임
    print(fish_input[:5]) #5개 열 추출

    fish_target = fish['Species'].to_numpy() #타겟 데이터임

    from sklearn.model_selection import train_test_split
    train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state=42)  

    from sklearn.preprocessing import StandardScaler
    ss = StandardScaler()
    ss.fit(train_input)
    train_scaled = ss.transform(train_input) #표준점수로 변환함.
    test_scaled = ss.transform(test_input)
    ```

+ step 2. 로지스틱 회귀로 이진 분류 수행하기
    ```python
    bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt') #비교 연산자 사용
    train_bream_smelt = train_scaled[bream_smelt_indexes]
    target_bream_smelt = train_target[bream_smelt_indexes]
    
    from sklearn.linear_model import LogisticRegression
    lr = LogisticRegression()
    lr.fit(train_bream_smelt, target_bream_smelt)

    print(lr.predict(train_bream_smelt[:5]))
    #['Bream' 'Smelt' 'Bream' 'Bream' 'Bream']
    print(lr.predict_proba(train_bream_smelt[:5]))
    #[[0.99759855 0.00240145]
    #[0.02735183 0.97264817]
    #[0.99486072 0.00513928]
    #[0.98584202 0.01415798]
    #[0.99767269 0.00232731]]
    ```
    + 훈련 세트에서 도미와 빙어의 행만 골라냄
    + predict_proba()메서드는 클래스별로 확률값을 반환
    + 이 때 클래스는 알파벳 순으로 정렬됌
    + bream과 smelt 두가지의 이진 분류이기에 2가지 경우의 확률을 구함

    ```python
    print(lr.coef_, lr.intercept_)
    #[[-0.4037798  -0.57620209 -0.66280298 -1.01290277 -0.73168947]] [-2.16155132]

    decisions = lr.decision_function(train_bream_smelt[:5]) #양성 클래스에 대한 z값 계산
    print(decisions)
    #[-6.02927744  3.57123907 -5.26568906 -4.24321775 -6.0607117 ]

    from scipy.special import expit
    print(expit(decisions))
    #[0.00240145 0.97264817 0.00513928 0.01415798 0.00232731]
    ```
    + coef_와 intercept_로 계수 구함
    + 사이파이 라이브러리에 있는 시그모이드 함수 이용
    + ![KakaoTalk_20240811_175903088](https://github.com/user-attachments/assets/681b2168-94c9-482f-ab9d-c91edd3305b2)
+ step 3. 로지스틱 회귀로 다중 분류 수행하기
    ```python
    lr = LogisticRegression(C=20, max_iter=1000) #C를 통해 규제를 제어함, 이때 C값이 작을 수록 규제가 강해짐, 기본값 1
    #max_iter로 반복 횟수를 지정하며 기본값을 100
    lr.fit(train_scaled, train_target)
    
    print(lr.predict(test_scaled[:5]))

    print(lr.coef_.shape, lr.intercept_.shape)
    #(7, 5) (7,)
    ```
    + 다중 분류는 클래스마다 z값을 하나씩 계산
    + 가장 높은 z값을 출력하는 클래스가 예측 클래스가 됌
    ```python
    decision = lr.decision_function(test_scaled[:5]) #z값 계산
    print(np.round(decision, decimals=2))

    from scipy.special import softmax
    proba = softmax(decision, axis=1)
    print(np.round(proba, decimals=3))
    ```
    + 다중 분류에서는 *소프트맥스 함수*를 사용하여 7개의 z값을 확률로 변환
    + axis=1로 지정하여 각 행, 즉 각 샘플에 대해 소프트맥스를 계산
    + 전체 클래스에 대한 합이 1이도록 만듦
    + ![KakaoTalk_20240811_181711581](https://github.com/user-attachments/assets/c1b2e49d-7a22-40b8-8c45-cb38ee524a75)
    + ![KakaoTalk_20240811_181724767](https://github.com/user-attachments/assets/c9e12eb1-01d2-42a0-9c57-5947e31f6bcf)

## 04-02
### 들어가기
**점진적 학습**
+ 훈련한 모델을 버리지 않고 새로운 데이터에 대해서만 조금씩 더 훈련하는 것
+ 대표적인 알고리즘 : **확률적 경사 하강법**

**확률적 경사 하강법**
+ 확률적이란 말은 '무작위하게'의 기술적인 표현
+ 경사라는 말은 '기울기'
+ 하강법은 '내려가는 방법'
+ 가장 가파른 길을 천천히 조금씩 내려오는 것이 중요함(*경사 하강법*)
+ 훈련 세트에서 전체 샘플을 이용하지 않고 랜덤하게 하나의 샘플을 고른다(*확률적*)
+ 훈련 세트에서 랜덤하게 하나의 샘플을 선태갛여 가파른 경사를 조금씩 내려가고 이를 반복적으로 수행하여 모든 샘플들을 다 사용함

**에포크**
+ 확률적 경사 하강법에서 훈련 세트를 한 번 모두 사용하는 과정

**미니배치 경사 하강법**
+ 여러 개의 샘플을 사용해 경사 하강법을 수행하는 방식

![KakaoTalk_20240811_184832087](https://github.com/user-attachments/assets/d34c30de-58af-40ca-957b-289ffbcfde75)

**손실함수**
+ 어떤 문제에서 머신러닝 알고리즘이 얼마나 엉터리인지를 측정하는 기준
+ 어떤 값이 최솟값인지 모르기 때문에 가능한 많이 알아보고 만족할만한 수준이라면 산을 다 내려왔다고 인정해야함
+ 정확도가 듬성듬성하다면 경사 하강법을 이요해 조금씩 움직일 수 없고 산의 경사면은 *연속적*이어야함
+ 즉 미분 가능해야함

**로지스틱 손실 함수**
+ ![KakaoTalk_20240811_185714857](https://github.com/user-attachments/assets/f365bd93-f350-44fb-9530-dff943d7ce35)
+ 4개의 샘플 예측 확률 : 0.9,0.3,0.2,0.8
+ 4개의 샘플 정답 : 1,1,0,0
+ 에측 확률의 범위는 0~1사이인데 로그 함수는 이 사이에서 음수가 되므로 최종 손실 값은 양수가 됨
+ 로그 함수는 0에 가까울수록 아주 큰 음수가 되기 때문에 손실을 아주 크게 만들어 모델에 큰 영향을 미칠 수 있음
+ ![KakaoTalk_20240811_190523472](https://github.com/user-attachments/assets/b63ec089-fa3f-4539-add0-4f0e4d9055d8)

### 실습
+ step 1. SGDClassifier
    ```python
    from sklearn.linear_model import SGDClassifier
    sc = SGDClassifier(loss='log_loss', max_iter=10, random_state=42)
    sc.fit(train_scaled, train_target)
    #loss='log_loss'를 통해 로지스틱 손실 함수를 지정
    #max_iter는 수행할 에포크 횟수를 지정
    print(sc.score(train_scaled, train_target))
    print(sc.score(test_scaled, test_target))
    ```
    ```python
    sc.partial_fit(train_scaled, train_target) #모델을 이어서 훈련할 때 partial_fit()메서드 사용,  1 에포크씩 이어서 훈련할 수 있음
    print(sc.score(train_scaled, train_target))
    print(sc.score(test_scaled, test_target))
    ```
+ step 2. 에포크와 과대/과소적합
    ```python
    import numpy as np
    sc = SGDClassifier(loss='log_loss', random_state=42)
    train_score=[] #에포크마다 점수를 기록하기 위함
    test_score=[]
    classes = np.unique(train_target)
    
    for _ in range(0,300):
        sc.partial_fit(train_scaled, train_target, classes=classes)
        train_score.append(sc.score(train_scaled, train_target))
        test_score.append(sc.score(test_scaled, test_target))
    ```
    + 확률적 경사 하강법을 사용한 모델은 *에포크 횟수*에 따라 과소적합이나 과대적합 일어날 수 있음
        + 적은 에포크 -> 과소적합
        + 많은 에포크 -> 과대적합
    + ![KakaoTalk_20240811_192354643](https://github.com/user-attachments/assets/abb8a46a-f3b7-4988-8ae8-ab415ecec868)
    + 과대적합이 시작하기 전에 훈련을 멈추는 것을 *조기종료*라고 함


    ```python
    import matplotlib.pyplot as plt
    plt.plot(train_score)
    plt.plot(test_score)
    plt.show()
    ```
    + ![스크린샷 2024-08-11 193835](https://github.com/user-attachments/assets/a39d2401-0e0f-43f8-a137-8e94d91ec531)

    + 위의 그래프를 통해서 100이 적절한 반복횟수임을 알 수 있음
    ```python
    sc = SGDClassifier(loss='log_loss', max_iter=100, tol=None, random_state=42)
    sc.fit(train_scaled, train_target)
    print(sc.score(train_scaled, train_target))
    #0.957983193277311
    print(sc.score(test_scaled, test_target))
    #0.925
    ```
    + SGDClassifier는 일정 에포크 동안 성능이 향상되지 않으면 더 훈련하지 않고 자동으로 멈춤
    + tol 매개변수를 None으로 지정하여 자동으로 멈추지 않고 max_iter=100만큼 무조건 반복하도록 함
    + loss 매개변수의 기본값은 'hinge'