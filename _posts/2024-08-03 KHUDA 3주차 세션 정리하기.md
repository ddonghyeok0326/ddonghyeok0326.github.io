---
layout: post
title:  "KHUDA 3주차 세션 정리하기"
date:   2024-08-03 14:47:46 +0900
categories: jekyll update
---
# Chapter 03

## 03-01

### 들어가기

+ **지도 학습 알고리즘**
    1. 분류
    2. **회귀**

**회귀**란 클래스 중 하나로 분류하는 것이 아니라 임의의 어떤 숫자를 예측하는 문제(두 변수 사이의 상관관계를 분석하는방법)

**k-최근접 이웃 회귀**는 분류와 똑같이 예측하려는 샘플에 가장 가까운 샘플 k개를 선택하지만 샘플의 타깃은 임의의 수치이다. 이웃한 샘플의 타깃값의 평균을 통해 예측한다.



+ **과대적합**
    + 훈련 세트에서 점수가 굉장히 좋았는데 테스트 세트에서는 점수가 굉장히 나쁘다면 모델이 훈련 세트에 *과대적합*되었다고 함
    + 훈련세트에만 잘 맞는 모델이라 테스트 세트와 나중에 실전에 투입하여 새로운 샘플에 대한 예측을 만들 때 잘 동작하지 않을 것임
    + 모델을 덜 복잡하게 만들어야 함으로 k-최근접 이웃의 경우 k값을 늘린다
+ **과소적합**
    + 훈련 세트보다 테스트 세트의 점수가 높거나 두 점수가 모두 낮은 경우에 훈련세트에 *과소적합*되었다고 함
    + 모델이 너무 단순하여 훈련 세트에 적절히 훈련되지 않은 경우
    + 모델을 복잡하게 만들어야 함으로 k-최근접 이웃의 경우 k값을 줄인다
    + 훈련 세트가 전체 데이터를 대표한다고 가정하기 때문에 훈련 세트를 잘 학습하는 것이 중요함

### 실습
+ step 1. 데이터 준비 및 산점도 그리기
    ```python
    #http://bit.ly/perch_data 이용
    import numpy as np
    import matplotlib.pyplot as plt
    plt.scatter(perch_length, perch_weight)
    plt.xlabel('length')
    plt.ylabel('weight')
    plt.show() #농어의 길이가 커짐에 따라 무게가 늘어남
    ```
+ step 2. 2차원 배열로 훈련 세트와 테스트 세트로 나누기
    ```python
    from sklearn.model_selection import train_test_split
    train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state=42)

    #사이킷런에 사용할 훈련 세트는 2차원 배열이여야함(현재 1차원 배열)
    train_input = train_input.reshape(-1,1)
    test_input = test_input.reshape(-1,1)
    #행(row)의 위치에 -1을 넣고 열의 값을 지정(1로 지정함)해주면 변환될 배열의 행의 수는 알아서 지정이 된다는 뜻
    ```
    + reshape() 메서드는 쿠기가 바뀐 새로운 배열을 반환할 때 지정한 크기가 원본 배열에 있는 원소의 개수와 다르면 에러가 발생함

+ step 3.결정계수
    ```python
    from sklearn.neighbors import KNeighborsRegressor
    knr = KNeighborsRegressor()
    knr.fit(train_input, train_target)
    print(knr.score(test_input, test_target))
    ```
    + 회귀에서는 정확한 숫자를 맞힌다는 것은 거의 불가능함(예측하는 값이나 타깃 모두 임의의 수치라서)
    + 먼저 타깃과 예측한 값의 차이를 제곱 -> 타깃과 타깃 평균의 차이를 제곱한 값으로 나눔
    + 1에 가까울수록 정확도가 높음

+  step 4. mean_absolute_error를 통해 오차 계산
    ```python
    from sklearn.metrics import mean_absolute_error

    #예측값
    test_prediction = knr.predict(test_input)

    #테스트 세트에 대한 평균 절댓값 오차를 계산
    mae = mean_absolute_error(test_target, test_prediction)
    print(mae)
    ```
    + 결정 계수는 직감적으로 얼마나 좋은지 이해하기 어렵다.
    + mean_absolute_error는 타깃과 예측의 절댓값 오차를 평균하여 반환
+ step 5. 과소적합 없애기
    ```python
    knr.n_neighbors=3
    knr.fit(train_input, train_target)
    print(knr.score(train_input, train_target))

    print(knr.score(test_input, test_target))
    ```
    + 훈련세트로 score()메서드를 이용할 경우 결괏값이 테스트 세트로 이용할 때보다 낮음, 즉 과소적합이다
    + 이웃의 개수 k를 줄임
    + 이웃의 개수를 줄이면 훈련 세트에 있는 국지적인 패턴에 민감해지고 이웃의 개수를 늘리면 데이터 전반에 있는 일반적인 패턴을 따름