---
layout: post
title:  "NLP week2"
date:   2024-09-19 00:49:46 +0900
categories: jekyll update
---
# **The spelled-out intro to language modeling: building makemore**

## 1. reading and exploring the dataset

**Bigram Model**
- 간단하고 약한 모델이지만, 문자 예측에 좋은 출발점이다
- 문자 수열을 통해 뒤이어 오는 문자를 예측하는데, 모델은 한 번에 두 문자씩을 처리한다(지엽적이다.)
```python
words = open('names.txt','r').read().splitlines()
words[:10]
```
- 데이터셋은 개별 단어들이 포함되어 있으며, 단어가 나타난 순서는 해당 문자열 시퀀스의 통계 구조를 암시한다
- 하나의 단어가 주어지면 다음 단어를 연속적으로 예측한다

## 2. exploring the bigrams in the dataset
```python
for w in words[:1]:
    for ch1, ch2 in zip(w,w[1:]):
        print(ch1, ch2) #한 번에 두 문자씩, 단어 사이를 밀어서 반복할 것이다....?

->e m
->m m
->m a
```
Python에서는 문자열을 bigram로 쪼갤 수 있는 간단하고 유용한 방법 **zip**을 제공하는데, 예시로 'emma'라는 문자열의 bigram을 출력한다

## 3. counting bigrams in a python dictionary
```python
b={}
for w in words[:3]:
    chs = ['<S>']+list(w)+['<E>']
    for ch1, ch2 in zip(chs, chs[1:]):
        bigram=(ch1, ch2)
        b[bigram] =b.get(bigram,0)+1

print(b)

{('<S>', 'e'): 1,
 ('e', 'm'): 1,
 ('m', 'm'): 1,
 ('m', 'a'): 1,
 ('a', '<E>'): 3,
 ('<S>', 'o'): 1,
 ('o', 'l'): 1,
 ('l', 'i'): 1,
 ('i', 'v'): 1,
 ('v', 'i'): 1,
 ('i', 'a'): 1,
 ('<S>', 'a'): 1,
 ('a', 'v'): 1,
 ('v', 'a'): 1} #a로 끝나는 경우가 3번 있었다
```
```python
#전체 데이터셋 사용
b={}
for w in words:
    chs = ['<S>']+list(w)+['<E>']
    for ch1, ch2 in zip(chs, chs[1:]):
        bigram=(ch1, ch2)
        b[bigram] =b.get(bigram,0)+1

sorted(b.items(), key = lambda kv: kv[1]) #value값을 오름차순으로 나열하기
#n으로 끝날 때가 제일 많았다
```
- 얼마나 자주 조합이 발생하는지 세는 방법
- **dictionary**를 사용하고 items를 사용하여 value값을 오름차순으로 나열할 수 있다

## 4. counting bigrams in a 2D torch tensor("traing the model")
- 2d array에 저장할 것이다
    - 행은 첫 번째 문자
    - 열은 두 번째 문자
- pytorch는 딥러닝 신경망 네트워크 framework임
- torch.tensor 다차원 array를 만들수 있고 사용을 효과적으로 가능

```python
import torch
N = torch.zeros((28,28), dtype=torch.int32) # 횟수를 표시하기 위해서 dtype=torch.int32 사용 #알파벳이 26개고 special 알파벳이 2개 더 있어서 28개임

sorted(list(set(''.join(words)))) #set은 중복을 허용하지 않는다

chars = sorted(list(set(''.join(words))))
stoi = {s:i for i,s in enumerate(chars)}
stoi['<S>'] = 26 #special character에 번호를 직접 추가
stoi['<E>']= 27

for w in words:
    chs = ['<S>']+list(w)+['<E>']
    for ch1, ch2 in zip(chs, chs[1:]):
        ix1=stoi[ch1]
        ix2=stoi[ch2]
        N[ix1, ix2]+=1

itos = {i:s for s,i in stoi.items()} #stoi의 역버전
```
## 5. sampling from the model
```python
N = torch.zeros((27,27), dtype=torch.int32)

chars = sorted(list(set(''.join(words))))
stoi = {s:i+1 for i,s in enumerate(chars)}
stoi['.'] = 0
itos = {i:s for s, i in stoi.items()}

for w in words:
    chs = ['.']+list(w)+['.']
    for ch1, ch2 in zip(chs, chs[1:]):
        ix1=stoi[ch1]
        ix2=stoi[ch2]
        N[ix1, ix2]+=1

N[0, :].shape #첫번째로 시작되는 문자열 횟수 뽑기
#torch.Size([27])
```
+ 위의 코드는 첫 번째로 오는 알파벳의 **횟수**를 구한것임

```python
# 횟수말고 샘플의 확률을 구하고자함
p = N[0].float() #확률을 구하기 위해서 실수 형태로 변환
p = p / p.sum()
p

p.sum()
#tensor(1.)
```
```python
g = torch.Generator().manual_seed(2147483647) #random seed를 정해주었다
p = torch.rand(3, generator =g) #같은generator를 사용
p = p/p.sum()
p
#0일 확률 약 60퍼, 1일 확률 약 30퍼, 2일 확률 약 9퍼
# tensor([0.6064, 0.3033, 0.0903])

torch.multinomial(p, num_samples=100, replacement=True, generator=g) #replacement = true로 설정하여 샘플링될 때마다 다시 선택될 수 있도록 한다.
# 결정론적인 결과를 얻는다
```
+ torch.multinomial 메소드를 사용해 이 분포에서 샘플링 가능하다
```python
g = torch.Generator().manual_seed(2147483647)

ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item() #index뽑아내기
itos[ix]
#'j'
```
+ 첫 번째 알파벳 구하기

```python
g = torch.Generator().manual_seed(2147483647)

out=[]
ix = 0
while True : 
    p = N[ix].float()
    p=p/p.sum()
    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()
    out.append(itos[ix])
    if ix == 0 :
        break
print(''.join(out))
#반복문을 작성해서 이름을 구함
#junide.
```
+ 인덱스 0부터 시작하여 while문을 통해 현재 인덱스에 해당하는 행을 가져옴
+ 가져온 행을 float 형태로 변환하여 정규화한다. 이를 통해 다음 인덱스가 될 확률을 얻는다
+ 다음 인덱스가 0이라면 종료 토큰이므로 종료한다. 그렇지 않으면 생성된 문자를 출력하고, 이 과정을 몇 번이나 반복하여 이름을 생성한다
+ 이름이 되기에 이상한 것들이 많다

## 6. efficiency ! vectorized normalization of the rows, tensor broadcasting
```python
P = N.float()

P.sum(1,keepdim=True).shape
#torch.Size([27, 1])

P=P/P.sum(1, keepdim=True)

g = torch.Generator().manual_seed(2147483647)
for i in range(5):

    out=[]
    ix=0
    while True : 
        p=P[ix]
        ix=torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()
        out.append(itos[ix])
        if ix==0:
            break
    print(''.join(out))

#junide.
#janasah.
#p.
#cony.
#a.
```
+ 행렬의 각 행을 반복할 때마다 불필요한 연산을 수행하는 비효율적인 과정을 개선하고자 한다.
+ 각 행이 1로 정규화된 확률 분포인 행렬 p를 준비하고, 이를 사용하여 연산하는 것이 목표
+ 축을 따라 요소들의 합을 계산하기 위해서 torch.sum의 'keepdim' 인자 활용 필요

## 7. loss function(the negative log likelihood of the data under our model)
```python
#GOAL : maximize likelihood of the data w.r.t. model parameters(statistical modeling)

#equivalent to maximizing the log likelihood (because log is monotonic)

#equivalent to minimizing the negative log likelihood

#equivalent to minimizing the average negative log likelihood

log_likelihood=0.0
n=0

for w in words[:3]:
    chs = ['.']+list(w)+['.']
    for ch1, ch2 in zip(chs, chs[1:]):
        ix1=stoi[ch1]
        ix2=stoi[ch2]
        prob = P[ix1, ix2]
        logprob = torch.log(prob)
        log_likelihood+=logprob
        n+=1
        #print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')

print(f'{log_likelihood=}') #로그 가능도
nll=-log_likelihood #음의 로그 우도
print(f'{nll=}')
print(f'{nll/n}')

#log_likelihood=tensor(-38.7856)
#nll=tensor(38.7856)
#2.424102306365967
```
+ 최소제곱법과 통계 모델링에서는 가능도를 사용하여 모델이 학습한 전체 데이터 집합의 확률을 나타낸다.
+ 이 가능도는 확률들의 곱으로, 모델 학습 시 이 값이 가능한 한 최대가 되어야 한다. 따라서 좋은 모델일수록 이 확률들의 곱이 매우 크게 나타난다.(이 확률들의 곱은 계산하기 어렵기 때문에 일반적으로 로그 가능도를 사용)
+ 로그 가능도(로그 우도도)
    + 확률에 자연로그를 취하는 것으로, 확률이 1에 가까울수록 로그 가능도는 0에 가까워지고, 낮은 확률일수록 로그 가능도는 음의 무한대로 작아진다.
    + 연산이 용이하고, 결과의 해석도 편리

+ 로그 가능도로 손실 함수를 계산할 수 있다.(낮을수록 성능이 좋다)
+ 로그 우도의 값을 음수로 바꾸어 '음의 로그 우도'라고 한다. 
+ 모델의 훈련 세트의 손실 함수는 2.4로 평가되었다.

## 8. model smoothing with fake counts
```python
P = (N+1).float()

for w in words["andrejq"]:
    chs = ['.']+list(w)+['.']
    for ch1, ch2 in zip(chs, chs[1:]):
        ix1=stoi[ch1]
        ix2=stoi[ch2]
        prob = P[ix1, ix2]
        logprob = torch.log(prob)
        log_likelihood+=logprob
        n+=1
        #print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')

print(f'{log_likelihood=}') #로그 가능도
nll=-log_likelihood #음의 로그 우도
print(f'{nll=}')
print(f'{nll/n}')
#log_likelihood=tensor(-27.8672)
#nll=tensor(27.8672)
#3.4834020137786865
```
+ 모델 스무딩을 적용하여 더 이상 무한한 확률이 아닌 다른 확률을 예측하도록 한다.
+ 원래는 jq가 무한한 확률을 가졌지만 P=(N+1).float()를 통해 다른 확률을 예측하였다.

## 9. Part 2: the neural network approach:intro
바이그램 언어 모델의 품질을 평가하기 위해 음의 로그 가능도를 사용하고, 이 값이 낮을수록 모델의 품질이 좋아짐.

문제를 **신경망 네트워크 프레임워크**로 바꾸어 접근하면서 기존 모델과 유사한 결과를 얻게 됨

신경망 네트워크로 구성된 문자 단위 언어 모델은 입력된 문자를 기반으로 다음 문자의 확률 분포를 출력함

입력된 문자를 기반으로 모델이 이 후 어떤 문자가 가능성이 높은지를 예상함. 


## 10. creating the bigram dataset for the neural net
```python
#creat the training set of bigrams (x,y)
xs, ys = [],[]

for w in words[:1]:
    chs = ['.']+list(w)+['.']
    for ch1, ch2 in zip(chs, chs[1:]):
        ix1=stoi[ch1]
        ix2=stoi[ch2]
        xs.append(ix1)
        ys.append(ix2)

xs=torch.tensor(xs)
ys=torch.tensor(ys)

print(xs)
# tensor([ 0,  5, 13, 13,  1]) # 인덱스를 제공한다.
print(ys)
# tensor([ 5, 13, 13,  1,  0])
```
+ 신경망의 파라미터 설정에 따른 모든 경우를 평가할 수 있으며, 손실 함수를 이용하여 확률 분포를 살펴본다.
+ 라벨들을 사용하여 다음 문자의 신원을 파악해 그에 따른 모델에서 할당되는 높은 확률을 확인한다.
+ 이를 위해 경사 기반의 최적화를 사용하여 파라미터를 조정하고 손실 함수를 최소화한다.
+ 첫 번째로 우리는 이 신경망의 훈련 세트를 컴파일하고 입력과 출력으로 구성된 빅램의 훈련 세트를 생성한다.

## 11. feeding integers into neural nets? one-hot encodings
```python
import torch.nn.functional as F
xenc = F.one_hot(xs, num_classes=27)
xenc

#tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])

xenc.shape
#torch.Size([5, 27])
```
+ torch.nn.functional 모듈의 F.one_hot 함수를 사용하여 정수를 one-hot 인코딩한 벡터를 얻을 수 있다.
+ 'one hot'함수는 특정한 dtype(데이터 타입)을 지원하지 않아서, 텐서의 dtype을 float으로 바꾸기 위해 'casting' 작업을 수행해야 함.

## 12. the "neural net": one linear layer of neurons implemented with matrix multiplication

```python
W = torch.randn((27,27)) #noraml distribution
xenc @ W

# (5, 27) @ (27,1) -> (5,27)
(xenc@W).shape
#torch.Size([5,27])
```
+ 27개의 뉴런을 가진 다수의 뉴런을 생성하려면, 기존 데이터의 shape(5 by 27)을 기반으로 가중치 행렬 'w'를 27 by 27로 바꿔야 함.
+ 행렬 연산을 통해 27개의 뉴런과 5개의 입력에 대한 결과를 동시에 업데이트할 수 있음.
+ 행렬 곱셈을 수행한 이후의 출력은 shape이 5 by 27임.

## 13. transforming neural net outputs into probabilities: the softmax

```python
(xenc@W).exp() #양수가 됌
logits = xenc @ W #log-counts
counts = logits.exp() #equvialent N
probs = counts / counts.sum(1,keepdims=True)
probs

#tensor([[0.0094, 0.0510, 0.0095, 0.0132, 0.0443, 0.0917, 0.0121, 0.0098, 0.0125, 0.0321, 0.0307, 0.0084, 0.1137, 0.0252, 0.0027, 0.1297, 0.0159, 0.0206, 0.0332, 0.0196, 0.0058, 0.0422, 0.0157, 0.0772, 0.0194, 0.1016, 0.0528],[0.0499, 0.0285, 0.1334, 0.0277, 0.0312, 0.0093, 0.0052, 0.0129, 0.0495, 0.0252, 0.0519, 0.0094, 0.0145, 0.0191, 0.0449, 0.0514, 0.0353, 0.1236, 0.0107, 0.0049, 0.0218, 0.1115, 0.0284, 0.0297, 0.0228, 0.0405, 0.0068],[0.0277, 0.0146, 0.0262, 0.0212, 0.0101, 0.3223, 0.0498, 0.0978, 0.0453, 0.0222, 0.0653, 0.0128, 0.0054, 0.0038, 0.0104, 0.0174, 0.0504, 0.0286, 0.0039, 0.0143, 0.0091, 0.0073, 0.0418, 0.0221, 0.0027, 0.0220, 0.0454],[0.0277, 0.0146, 0.0262, 0.0212, 0.0101, 0.3223, 0.0498, 0.0978, 0.0453, 0.0222, 0.0653, 0.0128, 0.0054, 0.0038, 0.0104, 0.0174, 0.0504, 0.0286, 0.0039, 0.0143, 0.0091, 0.0073, 0.0418, 0.0221, 0.0027, 0.0220, 0.0454],[0.0193, 0.0455, 0.0144, 0.0102, 0.0331, 0.0606, 0.0185, 0.1006, 0.0550, 0.0136, 0.0219, 0.0721, 0.0156, 0.0122, 0.0357, 0.0299, 0.0464, 0.0191, 0.0053, 0.0661, 0.0101, 0.0147, 0.0122, 0.0657, 0.0178, 0.0133, 0.1712]])

probs.shape
# torch.Size([5, 27])

probs[0].sum()
# tensor(1.)
```
+ 27개의 뉴런으로 이루어진 첫 번째 레이어로 27차원의 입력을 전달하고, 각 뉴런들은 입력값과 가중치를 이용한 행렬곱 연산을 수행한다.
+ 이 행렬곱 연산을 통해 얻은 결과값은 각 뉴런의 발화율(firing rate)을 의미하며, 이는 다음 문자의 확률 분포를 나타내기 위해 사용된다.
+ 이때 확률 분포를 구하기 위해 사용되는 값은 로그 카운트(log counts)이며, 이를 지수화함으로써 실제 카운트 값을 얻는다.
+ 지수 연산은 음수나 양수인 숫자를 취한다. 음수인 경우 항상 1보다 작은 값인 e^x를 얻는다.
    + 양수인 경우, 무한으로 커지는 수를 얻을 수 있다.
    + 이제 이 수들을 로그 카운트로 해석하고, 요소별로 지수화하여 양수는 더 커지고, 음수는 더 작아지는 형태로 바뀐 것을 볼 수 있다.
+ 이제 이제 우리는 이 확률분포를 예측하는 neural net을 가지게 되었다. 이제 이 예측은 카운트로 해석할 수 있는 양수값들로 존재한다.
+ 이러한 카운트 값들은 절대로 0보다 작을 수 없으며, W값에 따라 다양한 값이 존재할 수 있다.

## 14. vectorized loss
```python
torch.arange(5)
probs[torch.arange(5), ys] #ys로 인덱싱함

#위의 결과가
#probs[0,5], probs[1,13], probs[2,13], probs[3,1], probs[4,0]와 동일함
```
+ 신경망의 손실은 평균 제곱 오차로 계산되며, 손실 값은 신경망의 품질을 나타내는 단일 숫자이다.
+ 역전파를 통해 모든 매개변수의 기울기를 계산한 후, 경사도 반대 방향으로 파라미터를 갱신한다.
+ 분류 작업에서는 평균 제곱 오차 대신 음의 로그 우도를 사용하여 손실을 평가한다.
+ 효율적으로 확률을 가져오기 위해 인덱스를 사용하는 방법 중 하나는 PyTorch에서 `torch.range`를 사용하여 범위를 생성하는 것이다.
