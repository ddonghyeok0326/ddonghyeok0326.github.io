---
layout: post
title:  "[스터디]파이썬을 이용한 데이터 사이언스 & 시각화 스터디 week6"
date:   2024-11-23 14:43:46 +0900
categories: jekyll update
---
# 파이썬 데이터 사이언스 핸드북

##  5.3 초모수와 모델 검증

### 모델 검증에 대한 고려사항

1. 모델과 모델의 초모수를 선택

2. 훈련 데이터 일부에 이를 적용하고 예측값을 알려진 값과 비교해서 모델이 얼마나 효율적인지 추정

**검정 표본 사용(모델의 성능을 제대로 알기위해)**

+ 즉, 모델의 훈련 데이터에서 데이터의 일부를 빼내 모델 성능을 확인하는 검정표본(train_test_split사용)

```python
from sklearn.datasets import load_iris
iris = load_iris()
X=iris.data
y=iris.target

from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=1)

from sklearn.model_selection import train_test_split
#데이터를 각각 50%로 나눔
X1, X2, y1, y2 = train_test_split(X, y, random_state=0,
                                  train_size=0.5)

# 모델을 이 가운데 하나의 데이터 집합에 적용시킴
model.fit(X1, y1)

# 모델을 두 번째 데이터 집합으로 검증
y2_model = model.predict(X2)

from sklearn.metrics import accuracy_score
accuracy_score(y2, y2_model)
#0.9066666666666666
```


**교차 검증을 통한 모델 검증**
+ 검정 표본을 사용하면 모델을 훈련시킬 데이터의 일부 손실
+ 해결하기 위해 교차 검증
+ 즉, 데이터의 각 하위 집합이 훈련 자료와 검증자료로 사용되도록
+ ![KakaoTalk_20241123_145737291](https://github.com/user-attachments/assets/907beecb-b280-4433-a929-5849281340b2)

```python
from sklearn.model_selection import cross_val_score
cross_val_score(model, X, y, cv=5) #데이터를 5그룹으로 나눠 검증 시행을 할 때마다 차례대로 하나씩 모델 평가
```

### 최고의 모델 선택하기

**검증 곡선**
+ 훈련 표본 점수와 검정 표본 점수를 계산해줌
+ ![KakaoTalk_20241123_150304298](https://github.com/user-attachments/assets/f743f974-d3fe-427d-ae3c-8e48951f75e3)
+ 훈련점수는 검증 점수보다 높다
+ 모델 복잡도가 너무 낮으면 훈련 데이터가 과소적합
+ 모델 복잡도가 너무 높으면 훈련 데이터가 과대적합
+ 중간값에서 검증 곡선은 최댓값 -> 편향과 분산 사이의 적절한 트레이드 오프가 이루어졌다

```python
from sklearn.model_selection import validation_curve
degree = np.arange(0, 21)
train_score, val_score = validation_curve(PolynomialRegression(), X, y,param_name='polynomialfeatures__degree', param_range=degree, cv=7)

plt.plot(degree, np.median(train_score, 1), color='blue', label='training score')
plt.plot(degree, np.median(val_score, 1), color='red', label='validation score')
plt.legend(loc='best')
plt.ylim(0, 1)
plt.xlabel('degree')
plt.ylabel('score');
```
+ validation_curve를 사용하여 모델과 데이터, 모수 이름, 탐색 범위가 주어지면 함수가 자동으로 그 범위 내에서 훈련 점수와 검증 점수를 계산함
+ 이를 통해 최적의 트레이드오프가 3차 다항식임을 알 수 있음
+ ![스크린샷 2024-11-23 153339](https://github.com/user-attachments/assets/71041a69-2279-441a-a1e7-da713a0859bb)

**학습곡선**
+ 최적의 모델은 훈련 데이터의 규모에 의존하기도함
+ 학습곡선이란 훈련 집합의 크기에 따른 훈련점수/검증 점수의 플롯

+ ![KakaoTalk_20241123_151822556](https://github.com/user-attachments/assets/00b50e49-a0d6-4d33-a2be-ead39ba30f16)
+ 해당 복잡도의 모델은 작은 데이터세트를 과대적합함
+ 해당 복잡도의 모델은 큰 데이터세트를 과소적합함
+ 모델은 결코 훈련 집합보다 검증 집합에 더 좋은 점수를 주지 않음(즉, 교차하지 않음)
+ 특징으로 훈련 표본의 개수가 커질수록 특정 점수로 수렴함
    + 즉, 훈련 데이터를 더 늘리는 것은 도움이 되질 않음

## 5.4 특징 공학

**특징 공학**이란 문제에 대해 가지고 잇는 정보를 모두 취해 특징 행렬을 구축하는데 사용할 수 있는 숫자로 변환하는 것

### 범주 특징
+ 범주 데이터는 비수치 데이터의 일반적인 유형

```python
data = [
    {'price': 850000, 'rooms': 4, 'neighborhood': 'Queen Anne'},
    {'price': 700000, 'rooms': 3, 'neighborhood': 'Fremont'},
    {'price': 650000, 'rooms': 3, 'neighborhood': 'Wallingford'},
    {'price': 600000, 'rooms': 2, 'neighborhood': 'Fremont'}
]
```
```python
#{'Queen Anne': 1, 'Fremont': 2, 'Wallingford': 3}로 나타내고 싶음

#해당 범주의 유무를 각각 1이나 0으로 나타내는 추가 열을 생성하는 원-핫 인코딩 사용하기
#Scikit-Learn의 DictVectorizer이 지원함
from sklearn.feature_extraction import DictVectorizer
vec = DictVectorizer(sparse=False, dtype=int)
vec.fit_transform(data)
```
+ ![스크린샷 2024-11-23 160029](https://github.com/user-attachments/assets/557981e2-58c2-4efc-b7ac-1dee73f097cb)

+ 지역 열이 세 개의 레이블을 나타내는 세 개의 분리된 열로 확장됐고 각 행에는 그 지역과 관련된 열에 1이라는 값이 들어있음

### 텍스트 특징
+ 텍스트를 대표 수치값의 집합으로 변환
+ 데이터를 인코딩하는 가장 간단한 방법 중 하나는 단어 세게를 이용하는 것

```python
sample = ['problem of evil',
          'evil queen',
          'horizon problem']
```
+ 이 데이터를 벡터화 하려면 단어를 나타내는 열을 만들어야함
+ scikit-learn의 countvectorizer가 지원함

```python
from sklearn.feature_extraction.text import CountVectorizer

vec = CountVectorizer()
X = vec.fit_transform(sample)
X

import pandas as pd
pd.DataFrame(X.toarray(), columns=vec.get_feature_names_out())
#dataframe을 사용하면 검사하기 더 쉬워짐

#output
	evil	horizon	of	problem	queen
0	1	0	1	1	0
1	1	0	0	0	1
2	0	1	0	1	0
```

### 유도 특징

```python
import numpy as np
import matplotlib.pyplot as plt

x = np.array([1, 2, 3, 4, 5])
y = np.array([4, 2, 1, 3, 7])
plt.scatter(x, y)
```
+ 위의 데이터로 x,y관계를 정확하게 설명하기 힘듬
    + 더 복잡한 모델 필요함
    + 데이터를 변환해서 모델에 유연성을 더 부여할 수 있는 특징 열을 추가해서 할 수 있음

```python
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=3, include_bias=False)
X2 = poly.fit_transform(X)
print(X2)

model = LinearRegression().fit(X2, y)
yfit = model.predict(X2)
plt.scatter(x, y)
plt.plot(x, yfit);
```
+ ![스크린샷 2024-11-23 160400](https://github.com/user-attachments/assets/2fc6fdec-5740-41eb-8f8c-0c7c69cf3139)

### 누락 데이터의 대체
+ 누락된 데이터(Nan)을 적절한 채움 값으로 대체

```python
#평균이나 중앙값, 최빈값을 사용하는 기본 대체 방식의 경우, Imputer클래스를 제공함
from sklearn.impute import SimpleImputer
imp = SimpleImputer(strategy='mean')
X2 = imp.fit_transform(X)
X2
```


**정리하면**
+ 누락된 값을 평균으로 대체
+ 특징을 이차 형태로 전환
+ 선형 회귀를 적합시킴


## 5.5 심화 학습 : 나이브 베이즈 분류

**나이브 베이즈 모델**
+ 대체로 매우 높은 차원의 데이터세트에 적합한 상당히 빠르고 간단한 분류 알고리즘을 가짐
+ 매우 빠르고 조정 가능한 모수가 매우 적기 때문에 분류 문제에서 빠르고 간편한 기준선으로 쓰기에 매우 유용함

**베이즈 분류**
+ 통계량의 조건부 확률 사이의 관계를 나타내는 방정식인 베이즈 정리를 기반
+ ![스크린샷 2024-11-23 161442](https://github.com/user-attachments/assets/e5a43284-0d0e-4d1c-864f-afa5290ac339)
    + 관측된 특징이 주어졌을 때 레이블의 확률을 구하는 것


### 가우스 나이브 베이즈
+ 가장 이해하기 쉬운 나이브 베이즈 분류 방식
+ 각 레이블의 데이터가 간단한 가우스 분포로부터 추출된다고 가정

![스크린샷 2024-11-23 181839](https://github.com/user-attachments/assets/19bd283a-3360-4e9a-96d2-d0e89a7b3c62)
+ 데이터가 차원 사이에 공분산이 없는 가우스 분포를 따른다고 가정하는것
+ 타원은 타원의 중심으로 갈수록 확률이 더 커짐
+ 각 클래스에 대해 이 생성 모델이 준비되면 모든 데이터 점에 대한 우도 P(features|L)을 계산할 수 있는 간단한 방법이 생긴 셈이니 신속하게 사후 확률을 계산해서 주어진 점에 대해 어느 레이블이 가장 확률이 높은지 결정할 수 있음
    + scikit-learn의 sklearn.naive_bayes.GaussianNB 추정기에 구현되어 있음


### 다항분포 나이브 베이즈
+ 특징이 간단한 다항분포로부터 생성된다고 가정함
+ 계수나 계수율을 나타내는 특징에 가장 적절함
+ 개념은 이전과 동일하지만 *최적합 다항식 분포*로 모델링한다는 점이 다름


**나이브 베이즈 모델의 장점**
1. 훈련과 예측이 매우 빠름
2. 간단한 확률 예측을 제공
3. 대체로 해석이 매우 쉬움
4. 조정 가능한 모수가 매우 적음

**나이브 베이즈 분류기가 잘 작동하는 상황**
1. 순진한 가정이 실제로 데이터에 부합할 때
2. 매우 잘 구분된 범주를 가진 경우, 모델 복잡도가 별로 중요하지 않을 때
3. 매우 고차원 데이터를 가진 경우, 모델 복잡도가 별로 중요하지 않을 때


## 5.10 심화학습 : 다양체 학습

+ PCA가 유연하고 빠르며 쉽게 해석 가능하지만, 데이터 내에 비선형적인 관계가 있을 때는 그렇게 잘 작동하지 X

**다양체 학습**
+ 결점을 해결하기 위해 고차원 공간에 내장된 저차원 다양체로 데이터세트를 설명하고자 하는 비지도 추정방식

### 다차원 척도법(MDS, Multidimensional Scaling)

**다차원 척도법**
+ x,y 좌표에서 거리 행렬을 계산하는 것은 간단하지만, 거리로부터 x,y좌표를 다시 변환하는 것은 어려움
+ 점 사이의 거리 행렬이 주어지면 데이터의 D-차원 좌표 표현을 복구함


**다양체 학습으로서의 MDS**
+ MDS추정기에 3차원 데이터를 입력하고 거리 행렬을 계산한 다음, 이 거리 행렬에 대한 최적의 2차원 임베딩을 결정하도록 요청
+ 결과는 원본 데이터의 표현을 복구

```python
model = MDS(n_components=2, random_state=1)
out3 = model.fit_transform(X3)
plt.scatter(out3[:, 0], out3[:, 1], **colorize)
plt.axis('equal');
```
+ ![스크린샷 2024-11-23 185154](https://github.com/user-attachments/assets/2cabac13-b32b-4d0b-bf36-35b2add761b0)

즉, *고차원*의 임베딩된 데이터가 주어졌을 때 다양체 학습이 데이터 내의 특정 관계를 보존하는 데이터의 *저차원* 표현을 찾는것

**비선형 임베딩 : MDS가 실패한 경우**
+ MDS가 실패하는 경우는 임베딩이 비선형일 때
    + 즉, 임베딩이 간단한 연산을 넘어서는 경우

```python
def make_hello_s_curve(X):
    t = (X[:, 0] - 2) * 0.75 * np.pi
    x = np.sin(t)
    y = X[:, 1]
    z = np.sign(t) * (np.cos(t) - 1)
    return np.vstack((x, y, z)).T

XS = make_hello_s_curve(X)

from mpl_toolkits import mplot3d
ax = plt.axes(projection='3d')
ax.scatter3D(XS[:, 0], XS[:, 1], XS[:, 2],**colorize);
```
+ ![스크린샷 2024-11-23 192627](https://github.com/user-attachments/assets/178c0140-08c9-4594-b136-9e4130132540)
+ 간단한 MDS 알고리즘을 적용하면 비선형 임베딩을 '풀'수 없고 임베딩된 다양체의 근본적인 관계를 추적할 수 X
+ ![스크린샷 2024-11-23 192829](https://github.com/user-attachments/assets/dc07fad3-ccc6-48a2-b586-13d7305045bf)
+ 2차원 선형 임베딩은 S자 곡선을 풀어내는 게 아니라 원본의 y축을 혼란스럽게 만듬

**비선형 다양체 학습 : 국소 선형 임베딩**
+ 이러한 문제를 해결하기 위함

![스크린샷 2024-11-23 193116](https://github.com/user-attachments/assets/4c650c43-6130-4952-ab36-9d86f7e3f531)
+ 그림에서 각 희미한 선은 임베딩에서 보존돼야 할 거리를 나타냄
+ 왼쪽 그림
    + MDS가 사용한 모델
    + 데이터세트에 있는 각 쌍의 점 사이의 거리를 보존
    + 두 점 사이에 그려진 모든 선의 길이를 적절히 보존하면서 이 데이터를 단조롭게 만들 방법이 없어서 실패

+ 오른쪽 그림
    + 국소 선형 임베딩(LLE)이라는 다양체 학습 알고리즘이 사용하는 모델
    + 이웃하고 있는 점들 사이의 거리만 보존


### 다양체 방식에 대한 몇 가지 생각

다양체 학습의 취약점
+ 다양체 학습에는 누락된 데이터를 처리하기 좋은 프레임워크가 X
+ 적절한 이웃의 수를 선택하기 위한 믿을 만한 정량적 방법이 X
+ 전역적으로 결과 차원의 최적의 개수를 정하기 어려움
+ 임베딩된 차원이 의미하는 바가 항상 분명하지 X

## 5.12 심화 학습 : 가우스 혼합 모델

### GMM 등장 배경 : k-평균의 약점

k-평균
+ 각 군집의 중심에 군집에서 가장 먼 점에 의해 정의된 반지름을 가지는 원을 위치시키는 것
+ 약점
    + 군집 모양에서 유연성이 떨어짐
    + 호가률적 군집 할당이 빈약

### E-M 단계 일반화하기 : 가우스 혼합 모델

가우스 혼합 모델
+ 어떤 입력 데이터세트라도 최적으로 모델링하는 다차원 가우스 확률 분포의 혼합을 구하려고 함
+ 기댓값-최대화 방식을 사용
    + 위치와 모양을 예측
    + 수렴될 때까지 다음 단계를 반복
        + E-단계 : 각 점에 대해 각 군집의 일원일 가능성을 인코딩한 가중치를 구함
        + M-단계 : 모든 데이터 점을 기반으로 가중치를 사용해 각 군집의 위치, 정규화, 모양을 업데이트함

### 공분산 유형 선택하기
+ covariance_type 옵션을 통해 설정
    + 기본값은 'diag' : 각 차원에 따라 군집의 크기를 독립적으로 설정할 수 있고 결과로 얻는 타원을 축을 따라 정렬
    + 'spherical' : 더 간단하고 빠른 모델, 군집의 모양을 모든 차원이 동일하도록 제한
    + 'full' : 더 복잡하고 계산 비용이 높은 모델, 각 군집이 임의의 방향을 가지는 타원형으로 모델링됨
![스크린샷 2024-11-23 210222](https://github.com/user-attachments/assets/3269bbd0-b39a-4d9c-95aa-142ce469aa16)

### 밀도 추정에 GMM 사용하기

gmm은 근본적으로 밀도 추정 알고리즘임
+ 즉, 어떤 데이터에 GMM을 적합한 결과는 데이터의 분포를 나타내는 생성 확률 모델임


## 5.13 심화 학습 : 커널 밀도 측정

밀도추정 :  D-차원 데이터셋을 가져다가 데이터를 추출할 수 있는 D차원의 확률 분포 추정하는 알고리즘

커널 밀도 측정(KDE) : 어떤 의미에서는 가우스 혼합 아이디어를 논리적 극단으로 가져가는 알고리즘

### KDE 등장 배경 : 히스토그램

밀도 추정은 데이터세트를 생성한 확률 분포를 모델링하는 알고리즘



    
